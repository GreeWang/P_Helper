# The Efficiency Spectrum Of Large Language Models: An Algorithmic Survey

TIANYU DING, Microsoft, tianyuding@microsoft.com TIANYI CHEN, Microsoft, tiachen@microsoft.com HAIDONG ZHU, University of Southern California, haidongz@usc.edu JIACHEN JIANG, Ohio State University, jiang.2880@osu.edu YIQI ZHONG, University of Southern California, yiqizhon@usc.edu JINXIN ZHOU, Ohio State University, zhou.3820@osu.edu GUANGZHI WANG, Microsoft, t-gwang@microsoft.com ZHIHUI ZHU, Ohio State University, zhu.3440@osu.edu ILYA ZHARKOV, Microsoft, zharkov@microsoft.com LUMING LIANG, Microsoft, lulian@microsoft.com Abstract - The rapid growth of Large Language Models (LLMs) has been a driving force in transforming various domains, reshaping the artificial general intelligence landscape. However, the increasing computational and memory demands of these models present substantial challenges, hindering both academic research and practical applications. To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs. This survey delivers a comprehensive review of algorithmic advancements aimed at improving LLM efficiency1. Unlike other surveys that typically focus on specific areas such as training or model compression, this paper examines the multi-faceted dimensions of efficiency essential for the end-to-end algorithmic development of LLMs. Specifically, it covers various topics related to efficiency, including scaling laws, data utilization, architectural innovations, training and tuning strategies, and inference techniques. This paper aims to serve as a valuable resource for researchers and practitioners, laying the groundwork for future innovations in this critical research area. Our repository of relevant references is maintained here.

CCS Concepts: - Mathematics of computing; - Computer systems organization; - Software and its engineering; - **Theory of**
computation; - **Applied computing**;
Additional Key Words and Phrases: Large Language Models, Artificial Intelligence, Computational Efficiency, Memory Efficiency, Data Utilization, Architecture Design, Training, Tuning, Inference, Software.

## 1 Introduction

Large Language Models (LLMs) [27, 113, 241, 308, 335], characterized by their massive scale of tens or even hundreds of billions of parameters [12, 23, 53], have become a central focus in the field of artificial intelligence. These models, exemplified by applications like ChatGPT [201] and Claude [2], have demonstrated impressive capabilities in a variety of general-purpose tasks, such as text summarization [309], translation [101], question answering [224], and even rudimentary coding [33], largely attributed to their expertise in natural language understanding. Although the exact mechanisms driving their exceptional performance remain elusive [348], it is widely believed that their large size endows them with emergent abilities [286] not observed in smaller models, and this is seen as a key step toward achieving Artificial General Intelligence (AGI) [4, 24].

While the large size of LLMs is crucial for their capabilities (see Figure 1), it also presents a significant drawback: their deployment is severely limited by high computational costs and memory requirements [273, 301, 346, 349]. The resources 1We have mostly included research works proposed prior to September 2023, along with selected contributions postdating this period. Given the extensive volume of literature in this field, it is possible that certain relevant studies may have been inadvertently missed. We welcome suggestions for additional references and are open to incorporating them in subsequent revisions.

Efficient LLM Algorithmic Survey, Nov, 2023, USA. Ding, Chen, et al.

![1_image_1.png](1_image_1.png)

(a) 8-billion model (b) 62-billion model (c) 540-billion model

![1_image_0.png](1_image_0.png)

needed for training these models are substantial, creating challenges in both resource allocation and model design. For example, the cost of exploring different architectures or strategies becomes prohibitive [335]. Furthermore, their large size makes them unsuitable for resource-constrained environments like edge devices, thus narrowing their range of applications [6]. This computational burden also confines the development of LLMs to large companies with abundant resources [23, 200, 215]. Many essential details, such as the data collection pipeline and training methodologies, remain proprietary, which hinders academic research and poses challenges for smaller companies. Additionally, the environmental impact of training these models is not to be overlooked, raising concerns about carbon emissions and ethical considerations [274, 276, 291]. Consequently, there is a growing emphasis on improving the *efficiency* of LLMs.

Motivated by this pressing need for more efficient LLMs, this survey aims to provide a comprehensive and up-to-date understanding of the subject. For the context of this paper, "efficiency" is defined as the optimization of computational and memory resources without compromising model performance. Adopting a holistic approach, we explore multiple dimensions of efficiency that are crucial for the end-to-end development of LLMs. These dimensions encompass data utilization, architectural designs, training and tuning strategies, and inference techniques, —essentially covering the entire pipeline of model development from algorithmic and software perspective.2 While there are existing surveys that focus on specific aspects of LLM efficiency, such as data [322], training [246, 340, 349], tuning [329], or inference [301, 346], they often do not provide a comprehensive view. Other works, like [273], have offered valuable insights into various efficiency aspects for Natural Language Processing (NLP), yet the rapidly evolving nature of the LLM field calls for an updated and comprehensive review. In contrast, our paper aims to present a more thorough and current overview of key methodologies and techniques that contribute to the development of efficient LLMs.

The remainder of this survey is organized as follows to offer a comprehensive understanding of the multiple facets of LLM
efficiency from the algorithmic perspective:
- Section 2 *Background* introduces the core concepts of LLMs and outlines the evaluation metrics pertinent to assessing their efficiency.

- Section 3 *Budget Efficiency* examines the role of predictive approaches like scaling laws in optimizing the performance of LLMs within given resource constraints.

- Section 4 *Data Efficiency* focuses on techniques for optimizing data utilization, thereby reducing resource consumption without compromising performance.

2Remark here that the advancements and achievements from hardware aspect are omitted in this survey.

![2_image_0.png](2_image_0.png)

The Efficiency Spectrum of Large Language Models: An Algorithmic Survey Efficient LLM Algorithmic Survey, Nov, 2023, USA.

- Section 5 *Architecture Efficiency* reviews innovative architectural designs, providing a detailed examination of how architecture influences efficiency.

- Section 6 *Training and Tuning Efficiency* discusses strategies for efficiently training LLMs from scratch and fine-tuning pre-trained models for specific downstream tasks.

- Section 7 *Inference Efficiency* explores the realm of model compression techniques designed to accelerate inference speed and reduce memory footprint.

- Section 8 *Conclusion* summarizes the key findings of this survey and discusses their broader implications for the advancement of efficient LLMs.

A schematic overview of these various dimensions of LLM efficiency is presented in Figure 2.

## 2 Background

In this section, we present an overview of the core concepts that form the basis of LLMs, along with the key metrics used for assessing their efficiency.

## 2.1 Core Concepts Of Llms

Language modeling, a cornerstone in the field of NLP, aims to model the generative likelihood of word sequences and predict the probabilities of subsequent or missing tokens. This area has evolved significantly over several decades. Initially rooted in statistical language models [15, 29, 118, 227, 323], the focus has gradually shifted to pre-trained neural language models [130, 156, 189, 190, 207, 215] and, more recently, to Large Language Models (LLMs) [27, 113, 241, 308, 335]. While there is no standardized definition for LLMs, they are typically distinguished by their extensive parameter sizes and extraordinary learning capabilities. In this section, we adopt the criteria outlined in [335], focusing on language models with more than one billion parameters, and discuss their core concepts in detail.

Architectural Foundations. LLMs can generally be categorized into two main paradigms: encoder-decoder models [149, 151, 175, 207, 217, 233], exemplified by BERT [130], and decoder-only models [12, 23, 53, 72, 107, 200, 214–216, 235, 269–
271, 293, 331], such as the GPT series [23, 200, 214, 215]. BERT is trained using masked language modeling, enabling it to excel in contextual understanding by predicting masked or missing tokens. On the other hand, GPT models are trained using autoregressive modeling, which equips them with strong generative capabilities by predicting subsequent tokens in a sequence.

Despite these differences, both types of models commonly rely on the Transformer [275] architecture, which is particularly noteworthy for its self-attention mechanism. In self-attention, each token is represented as a key, *value*, and *query*. The *query* weighs the importance of other tokens, represented as *keys* in understanding a particular token. These weights are applied to the *values* to create context-aware representations. This mechanism allows each token in the sequence to consider all other tokens simultaneously, facilitating parallel processing of sequential data and effective capture of long-sequence dependencies.

As a result, multi-head attention layers are often stacked to form deep networks in LLMs. Nowadays, decoder-only models like GPT-4 [200] and LLaMa [270, 271] are becoming increasingly prevalent, yet the core architectural module of self-attention remains a constant across these variations.

Training Essentials. LLMs acquire their general-purpose capabilities from an initial *pre-training* phase on expansive and diverse datasets [23, 308]. These datasets cover a broad spectrum of sources such as books, scientific papers, code, and websites [322]. This foundational knowledge is then fine-tuned on relatively smaller datasets in a supervised manner, aimed at enabling the LLMs to adhere to human instructions, a process known as *instruction tuning* [49, 54, 56, 170, 202, 257, 265, 284, 329, 338, 344, 351]. A *reinforcement learning with human feedback* is then proceeded to a instructed fine-tuned LLM to further align model behavior upon human preferences and instructions [271]. In the current landscape, decoder-only models have become the norm, particularly for their superior generative abilities. These models employ autoregressive objectives in the pre-training phase to maximize the likelihood of predicting subsequent tokens based on their preceding context. The scaling [107, 126] of this autoregressive pre-training significantly enhances LLM capabilities, as demonstrated by models like the GPT and PaLM series [12, 53]. For instance, PaLM [53] was trained on a 780B-token dataset and utilized a 540B-parameter Transformer architecture. PaLM-2 [12], its successor, advances this further; its largest variant has 1.1T parameters and was trained on a more diverse, multilingual dataset with 1.5T tokens. However, this scaling introduces its own set of challenges, requiring efficient training infrastructures and optimization strategies. Distributed training frameworks that focus on data parallelism, such as DeepSpeed [221] and Fully Sharded Data Parallel (FSDP) [3], or pipeline parallelism like Gpipe [114]
and PipeDream [195], are commonly employed. Additionally, tensor parallelism techniques like Megatron-LM [197, 250] and SARATHI [5] are also utilized. Specialized techniques, such as mixed-precision training [65, 194, 235] and quantization-aware training [168, 176, 296], are often incorporated to streamline the training process. These methods not only address the computational challenges tied to the scale of LLMs but also facilitate the development of increasingly capable models.

Versatile Abilities with Prompt Engineering. One of primary mechanisms for harnessing the versatility of LLMs across diverse tasks is through prompt engineering [172, 290, 343]. In this setting, a prompt consists of natural language instructions given by users to guide the LLM's behavior. The art of prompt engineering lies in crafting these instructions to elicit specific and contextually appropriate responses from the model. Two prominent techniques are few-shot [23, 287] and zero-shot [140]
prompting. Few-shot prompting provides the model with example tasks and corresponding solutions, while zero-shot prompting relies solely on task descriptions. This form of prompting is closely related to in-context learning (ICL), a concept first observed in GPT-3 [23], which allows the model to adapt to new tasks without retraining. The emergent abilities of LLMs, particularly in tackling a wide array of unseen tasks, are significantly enhanced when ICL is combined with well-designed prompts. Advanced prompting strategies like Chain-of-Thoughts (CoT) [283, 287], Tree-of-Thoughts (ToT) [178, 299, 310],
and Graph of Thoughts (GoT) [19] draw inspiration from human reasoning and cognitive structures. These strategies enable LLMs to explore novel capabilities like backtracking, thought merging, and idea elimination, thereby enhancing the quality of responses in complex reasoning tasks such as arithmetic [203], commonsense reasoning [263], and question answering [85].

As a result, prompt engineering serves as a pivotal mechanism for amplifying the versatility and effectiveness of LLMs.

## 2.2 Evaluation Metrics For Efficiency

Evaluating the efficiency of LLMs requires a multi-faceted approach that considers various performance indicators. These metrics are often presented alongside measures of accuracy and versatility to provide a holistic assessment of an LLM's overall efficiency and effectiveness. In the paragraphs that follow, we will explore key metrics commonly used to understand efficiency in the realm of LLMs.

Number of Parameters. The number of parameters in an LLM is a key factor that directly affects the model's learning capacity and complexity. These parameters, which include elements like weights and biases, are learnable during training or fine-tuning phases. A higher parameter count usually enables the model to grasp more complex data patterns, contributing to the development of various emergent abilities. However, this comes with the downside of increased computational demands for both training and inference. Additionally, having too many parameters can lead to overfitting, especially when the training data is scarce. To mitigate this, common techniques like regularization and early stopping are frequently used. Model Size. Model size, defined as the disk space required for storing the entire model, is often the initial consideration when training a new LLM or working with a pre-trained model. Given that exceedingly large models may be infeasible to store or run, this metric is particularly crucial for real-world deployments, especially in storage-constrained environments like edge devices. Expressed in units such as gigabytes (GB) or megabytes (MB), model size is influenced by several factors. While the number of parameters plays a significant role, other elements like the data type used for parameters (*e.g.*, float16, int8) and specific architectural choices also contribute. In addition to its direct impact on storage requirements, model size serves as an indirect indicator of the computational resources needed for both training and inference.

Floating Point Operations (FLOPs). Floating-point operations (FLOPs) is commonly used to gauge the computational complexity of LLMs. This metric counts the number of floating-point operations like addition, subtraction, multiplication, and division, giving an estimate of the computation done during a single forward pass. While FLOPs offer valuable insights into computational needs and potential energy use, they are not a complete measure. Other factors, such as system parallelism and architectural choices, also play a role in determining a model's overall computational efficiency. A higher FLOPs count usually means the model is more computationally demanding, which can be a challenge for deployment in environments with limited resources. As a result, optimizing this metric is often a key focus in the development of more efficient LLMs.

Inference Time / Tokens per Second. Inference time, also known as latency or delay, measures the duration it takes for an LLM to process input and generate a response during the inference stage. Unlike FLOPs, which provide a theoretical estimate of computational needs, inference time offers a practical gauge of real-world performance. This is because it's assessed in actual deployment settings, taking into account specific hardware and optimizations. Usually expressed in milliseconds (ms) or seconds (s), this metric is crucial for real-time applications that need quick responses or have stringent latency constraints.

Normalizing the inference time by time elapsed results in tokens per second, which refers to the number of tokens that a language model can process (read, analyze, generate, etc.) in one second. This is a key performance indicator that reflects the model's speed and efficiency. Achieving a balance between fast inference time / tokens per second and high generalization is a key focus in the development of efficient LLMs.

Memory Footprint. Memory footprint refers to the amount of Random Access Memory (RAM) required to load and run a model during inference or training. This metric is crucial for understanding the model's operational demands, especially in resource-constrained environments like edge devices or servers with limited memory capacity. Expressed in MB or GB, the memory footprint includes not just the model parameters but also other runtime necessities such as intermediate variables and data structures. A larger memory footprint can limit the model's deployability and may require optimization techniques like model pruning or quantization to reduce it.

Carbon Emission. Carbon emission is an increasingly important metric in the evaluation of large models, reflecting the environmental impact of training and running these models. This metric is usually measured in terms of kilograms or tons of CO2 equivalent emitted during the model's lifecycle, from training to inference. The carbon footprint is influenced by various factors, including the energy efficiency of the hardware used, the source of electricity, and the duration of model training and operation. High carbon emissions not only have environmental implications but can also affect the social and ethical considerations of deploying LLMs. As a result, there is a growing emphasis on optimizing models to be more energy-efficient, thereby reducing their carbon footprint. This is often achieved through hardware acceleration, algorithmic improvements, or even selecting greener energy sources for data centers.

## 3 Budget Efficiency: Scaling Laws 3.1 Introduction

The performance of large language models (LLMs) is significantly influenced by various factors, including training data, model size, architecture, computing resources, and the training methodology itself. Training LLMs requires extensive resources, making the conventional trial-and-error method for optimizing these factors both impractical and resource-intensive. As a result, predicting LLM performance before training is not just beneficial, but often necessary. This predictive approach allows for more effective planning and allocation of resources. For instance, consider a scenario with limited computing resources:
How can we optimally balance the model size and training data to achieve minimal objective function value? Answering such questions beforehand can significantly enhance the efficiency and effectiveness of LLM training processes.

Recent research in predicting large language model (LLM) performance has concentrated on understanding the scaling law [126]. This law delineates how LLM performance is influenced by factors such as model architecture, neural model size, computing power for training, and available data. The concept of scaling law, rooted in statistical mechanics approaches for predicting model generalization, has a rich history dating back to the early 1990s [10, 17, 96, 240]. Its relevance has been reinvigorated recently in the context of modern deep learning models [9, 25, 102–104, 107, 126, 192, 226, 254, 266, 268]. This section will delve into the latest advancements and insights in the scaling law as applied to LLMs, highlighting how these models evolve and perform under varying conditions.

## 3.2 Scaling Law

The work [126] presents a thorough study of the empirical scaling laws of transformer-based large language models. The authors observe that model performance (objective function ) primarily depends on three factors: the number of model parameters , dataset size , and the computing budget for training. They demonstrate a power-law relationship between model performance (measured in objective function, ) and these factors. For instance, they found that the relationship between performance and dataset size can be represented as () ≈ (5.4 × 1013/)
0.095. This formula suggests that as the dataset size increases, the model's performance improves following a specific pattern. While theoretical generalization bounds may suggest a similar power-law relationships, they generally do not provide specific coefficients like those identified in Kaplan et al.'s work [126]. This specificity is crucial for accurately predicting model performance. Additionally, the study highlights that transformers, known for their effective handling of long-range data dependencies, tend to outperform Long Short-Term Memory networks (LSTMs) [106] as they scale. This observation underscores the potential of transformers in large-scale language processing tasks.

Compute-Optimal Models via Scaling Law. When working within a fixed computational budget, it is crucial to find the right balance between model size () and dataset size (). This is where the scaling law curve, ( , ), becomes a vital tool. It helps determine the most effective trade-off between these two factors. The scaling law, as observed in [126], was instrumental in designing GPT-3, a 175 billion parameter language model. Interestingly, GPT-3 was trained on fewer tokens than was typical for its time [23]. Different forms of the scaling law curve have led to the development of diverse models, as seen in subsequent studies [9, 25, 107]. A notable application of these predicted scaling laws is found in [107]. Their research revealed that many previously trained LLMs, including Gopher [216], could have achieved better performance within the same compute budget. They demonstrated this by training a smaller model, Chinchilla, with 70 billion parameters, which outperformed the larger 280 billion parameter Gopher model [216] while using a similar compute budget.

Scaling Law for Transfer Learning. While the scaling behavior of pretrained LLMs has been extensively studied and exhibits clear predictability, it becomes less clear when predicting the performance of pretrained LLMs on downstream tasks.

The work in [103] investigates the scaling behavior when fine-tuning a pretrained model and demonstrates that favorable scaling laws, akin to those in [126], apply to transfer and few-shot settings in NLP tasks. In comparison to models trained from scratch, pretrained models exhibit a more advantageous scaling law in low-data scenarios. Different scaling behaviors between upstream and downstream configurations are observed in [268], indicating that, in addition to model size, the model's architecture plays a critical role in downstream fine-tuning. Specifically, the study demonstrates that redesigned models can attain similar downstream fine-tuning quality while having 50% fewer parameters and training 40% faster when compared to the widely adopted T5-base model [218].

Scaling Law in the Data-Constrained Regime. What if the training data are limited? The work [192] examines a dataconstrained regime and observes that, for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes in the objective function compared to a single epoch3. However, further increasing the number of epochs will lead to a decrease in the performance of the learned model.

3Unlike general machine learning models, which are typically trained with multiple epochs, it is common practice to train Language Models (LLMs) using a couple of epochs [23, 141].

Effect of Data Quality. A pivotal question in the realm of machine learning is whether the quality of data can lead to a transition from power-law to exponential scaling in model performance. The work by [254] provides an intriguing insight into this matter. They demonstrate that for certain vision classification tasks, the objective function can exhibit exponential scaling with an increase in dataset size, deviating from the traditional power-law scaling observed with pruned datasets.

While this phenomenon is initially observed in vision tasks, recent research, including works by [74, 93, 166], expands this concept to other domains. These studies explore the impact of high-quality data in tasks like generating coherent English, coding, and common sense reasoning. They suggest that high-quality data can significantly alter the scaling laws' trajectory.

This change indicates the potential for more efficient models, which, despite being trained on fewer data tokens yet with high quality, could match the performance of large-scale models trained on vast datasets without sufficient quality constraints.

This shift in understanding the role of data quality could revolutionize the approach to training and optimizing LLMs.

Effect of Architecture. In the domain of model scaling, conventional wisdom, as supported by studies like [102, 126],
suggests that the inherent attributes of models, such as the width or depth of Transformers, have a minimal impact on performance. However, the work by [266] presents a contrasting viewpoint. This study delves into the influence of different architectural designs on the scaling law and reveals that architecture plays a crucial role in scaling processes. Tay et al. [266] demonstrate that depending on the scale, the most effective model architecture can vary significantly. This finding complements previous assumptions and underscores the importance of considering architectural variations in the quest for optimal model performance at different scales.

## 4 Data Efficiency 4.1 Introduction

The insatiable demand for data by large-scale models has significantly fueled the growth of the data collection and preparation industry. However, this reliance on vast datasets, often accumulated over years, introduces substantial challenges in model training. These include not only prolonged training durations but also escalated costs due to extensive power consumption and the need for larger data storage capacities. Consequently, finding ways to use data more efficiently in both training and validation phases is of paramount importance. In this section, we will delve into strategies and considerations for enhancing data efficiency, addressing how to maximize the utility of data while mitigating the associated costs and resource demands.

## 4.2 Data Filtering

Data filtering is pivotal in directing training focus towards more informative samples, thereby eliminating irregular characters or patterns, rather than concentrating on examples with lesser informational value.

Deduplication. A prime data filter is removing duplications, i.e., *deduplication*. This straightforward yet efficacious approach not only abbreviates training duration but also enhances model performance, as evidenced by [152]. The utility of this de-duplication operation is evident at both the pre-training and fine-tuning stages of model development. In both pre-training and fine-tuning, the researchers utilizes techniques such as MinhashLSH [154], CC-NET [289], and adversarial filtering [321], as demonstrated by [22, 191, 331], for purging duplicates from the training datasets.

Data Undersampling. Beside deduplication, *data undersampling*, also referred to as instance selection, emerges as another promising data filtering technique [322]. This approach aims to reduce the volume of training samples by sub-sampling large datasets, yet crucially retains the distribution characteristics of the original training data. Moreover, this sub-sampling process can help mitigate issues of data imbalance. For instance, Prusa et al. [213] demonstrated the effectiveness of random undersampling in majority classes, which serves dual purposes: it reduces redundancy in the training set and balances the data distribution across various classes. Additionally, MESA [177] represents a novel advancement in this area by adopting Meta Learning to learn how to undersample vast datasets effectively. These techniques highlight the strategic importance of selective data use, not only in terms of quantity but also in ensuring quality and balance.

## 4.3 Active Learning / Importance Sampling

Active Learning or importance sampling helps a machine learning algorithm achieve better or equivalent performance with fewer annotated training samples. Their applications in training with extensive datasets predates the emergence of LLMs [127, 239]. These methodologies strategically reduce the total number of training samples by applying various criteria.

They aim to optimize the data collection and selection procedure by selecting and annotating only the most useful instances during the data annotation procedure [237, 238, 246]. The essence lies in the ability to prioritize samples based on their significance to the learning process, thereby optimizing the training efficiency for models dealing with large-scale data.

Gradient Norm. Katharopoulos and Fleuret [128] introduced an approach based on the upper bound of the gradient norm, along with proposing an estimator for variance reduction through importance sampling. Furthermore, Zhang et al. [326] implemented a selector mechanism to identify samples with larger gradients within a batch, treating them as valid approximations for importance sampling. According to their methodology, samples with smaller gradients are deemed 'good points' in a given epoch, necessitating increased focus on 'bad points', *i.e.*, samples with larger gradients. Upon these paradigm, training and fine-tuning over the samples with larger gradients could enhance the model performance more effectively. Objective Function / Predicted Score. Beyond the gradient, Katharopoulos and Fleuret also suggested in a separate work [127] that the objective function (loss value) itself could serve as a viable metric for importance sampling. Expanding on this concept, Jiang et al. [121] introduced 'selective back-propagation'. This method accelerates training by omitting the backpropagation stage for training samples exhibiting low loss, a strategy claimed to enhance convergence and outpace traditional Stochastic Gradient Descent (SGD) on full datasets. In the context of text retrieval, a similar *relevance sampling* [82, 232]
has been proposed. It leverages the scores predicted by current model to evaluate the relevance of the samples and let the annotators only annotate those with higher scores. Such methods may focus on the samples with high scores thereby resulting in overfitting. Some researchers turn to *uncertainty-based sampling* methods [81, 155, 251, 264, 279]. This thread of methods defines that the instances with lower uncertainties are more useful for improving performance and are worthier of annotation. Diversity Sampling. In contrast to the sampling methods which find difficult examples to train and annotate, methods that follow the strategy of *diversity sampling* [21, 87, 112, 236, 255, 305, 330] want to enhance the heterogeneity of training data.

For diversity sampling, there are two major approaches: iterative selection and cluster-based selection. Methods that belong to iterative selection iteratively examine if each instance can help improve the training data diversity and only annotate qualified instances. For example, Vote-k proposed in [255] adopts the cosine similarly between the embedding of each unlabeled instance and its k-nearest neighbors to define instances representativeness; ALLSH [330] represents the diversity according to the data instance's local sensitivity [28]. Cluster-based methods will cluster unannotated datasets and select instances based on the cluster labels. Methods in this category have explored many clustering strategies for instance samplings, such as
(weighted) K-means [31, 184, 315, 336].

Hybrid Sampling. Gradient or objective-function based sampling necessitates a relatively reliable initial Large Language Model (LLM), as sample selection depends on the model's predicted outputs, a process often termed a *warm start* [316].

On the other hand, diversity sampling can accommodate a *cold start* scenario. However, this approach may inadvertently introduce outliers, potentially compromising the final model's performance. To address the trade-off between both sampling threads, recent researches have shifted towards *hybrid sampling methods* that integrate both elements to surmount the challenge [13, 55, 136, 185, 252, 316]. For instance, Yuan et al. [316] incorporated BADGE [13] into language processing. This method involves transforming unannotated instances into representations that encapsulate model confidence, followed by clustering these transformed instances. In parallel, Margatina et al. [185] introduced Contrastive Active Learning (CAL). CAL selectively acquires contrastive samples from the pool of raw unannotated data, defining them as instances that are proximate in the model's feature space (*e.g.*, sharing similar vocabulary or model encodings) but yield divergent predictive likelihoods.

Experimental results affirm that CAL realizes a more effective balance compared to individual strategies, showcasing its potential in refining sampling processes.

Other Sampling. In addition to the above techniques, there exists a spectrum of sampling methods that operate on a broader scale. Chen et al. [45] introduced a novel approach by masking parameters to halt the back-propagation from certain inputs.

This method preserves the forward graph, ensuring that the statistical characteristics of all batches are maintained, especially in layers such as normalization, thereby reflecting the overall dataset's mapping. Taking a different approach, Xie et al. [298]
implement importance sampling in a reduced feature space. This strategy facilitates the tractability of importance weight estimation across the expansive text space, leveraging Kullback-Leibler (KL) reduction to efficiently train Large Language Models. Moreover, Sujit et al. [258] advocate for clipping training samples based on their relevance in reinforcement learning contexts, prioritizing samples according to their importance. Yao et al. [311] propose a keyword-based retrieval system for selecting relevant training samples in semi-supervised language fine-tuning, usually picking the top-k neighbors for each target example during training. These diverse methodologies highlight the evolving landscape of importance sampling, extending its application beyond standard batch-centric approaches to encompass broader training strategies for LLMs.

## 4.4 Curriculum Learning

Curriculum learning [18, 75] is a strategy that aims to improve the model training efficiency by carefully designing the feeding order of the instances in the training data. The principle of this approach is to initiate training with simpler samples or subtasks and progressively escalate to more challenging ones. Two critical components are integral to the design of a curriculum learning method. (i) The *difficulty metric* (or difficulty criterion), responsible for ranking the training samples based on their complexity. This metric serves as a guide to categorize training instances from the simplest to the most complex.

(ii) The *pacing function* (also known as curriculum scheduler or arrangement), which determines the rate at which these ranked samples are fed to the model training. This function modulates the learning curve, ensuring that the model is not overwhelmed by the complexity of the tasks too early.

Difficulty Metric. In the realm of curriculum learning for natural language processing, the most widely used difficulty metric is perhaps the sequence length [84, 157, 158]. The underlying assumption is that processing longer sentences poses greater challenges than shorter ones. Another prevalent metric is vocabulary rarity [285, 334], based on the intuition that sentences with less frequently used words in the training set are inherently more complex to comprehend. In addition, this metric could be measured by the uncertainty sampling principle in active learning, where the uncertainty indicated by other pre-trained models could serves as a gauge of difficulty as well.

For fine-tuning in specific downstream tasks, researchers have innovated task-specific difficulty metrics. A notable example is in paraphrase generation, where Kadotani et al. [124] proposed using the edit distance between paraphrased sentence pairs as a metric, approximating the extent of required transformations. These custom metrics, tailored to the nuances of specific tasks like paraphrase generation, often outperform general metrics such as sentence length or word rarity. In the context of neural machine translation (NMT), defining sample difficulty is more complex. In addition to sentence length and word rarity, some studies [139] have incorporated linguistically-motivated features. For instance, the number of coordinating conjunctions in a target sentence has been used to estimate sentence complexity. Such linguistically-informed metrics provide a more nuanced understanding of difficulty in tasks where traditional metrics may fall short, reflecting the evolving landscape of curriculum learning in NLP. Pacing Function. In the curriculum learning, the pacing function plays a crucial role in dictating the progression of training complexity. A common approach involves utilizing predefined step-wise functions, such as linear, root, or exponential curves.

Typically, this process starts by defining the total training steps  , the highest difficulty level  and the lowest difficulty level . The difficulty level for each training step  is then determined by the chosen curve. Apart from step-wise methods, stage-wise pacing methods offer an alternative. An example of this is Shortformer [211], which employs a two-stage approach.

In the first stage, it exclusively trains on short sequences, while in the second stage, it focuses solely on longer sequences, with a fixed length of 3072 tokens in the Shortformer model.

Beyond the separate determination of difficulty metrics and pacing functions, self-paced learning [145] presents a more integrated strategy. This approach involves simultaneously selecting easier samples and learning a new parameter vector in each iteration. The number of samples chosen is regulated by a weight that gradually increases, eventually encompassing the entire training dataset. This method has found applications in fields like machine translation [277] and dialogue generation
[345], showcasing its versatility and effectiveness in various NLP tasks. Curriculum learning has been successfully adopted to improve the data efficiency of many LLM downstream pretraining and finetuning procedures. However, how to choose the difficulty metric and pacing speed is non-trivial and requires experimental investigation for certain tasks or models.

## 5 Architecture Efficiency 5.1 Introduction

Recently, the Transformer family [275] has been the dominant architecture for language modeling, owing to its strong parallelism over recurrent methods such as RNNs [189]. However, its substantial computational cost renders the overall architecture inefficient in processing and handling long inputs. In particular, one of the key operations in the Transformer architecture is the attention mechanism. It typically requires quadratic complexity with respect to sequence length for computation and is thus significantly slow when processing long text inputs [100]. Reducing the computation required by the attention operation [267] becomes a direct solution to improve the architecture's efficiency, benefiting both the training and inference stages. Toward this end, researchers are exploring solutions for more *efficient attention* [50, 58, 60] along with different types of *positional encoding* [47, 48, 129, 162, 206, 212, 229, 256], or leveraging the inherent sparsity within the model to avoid activating all parameters during the feedforward computation with *sparse modeling* [72, 248]. Additionally, some recent works have directly replaced the attention mechanism with alternative architectures, introducing *attention-free* methods [61, 204, 210, 260] to the fold. In this section, we cover these four primary directions with the latest progress.

## 5.2 Efficient Attention

The Transformer model, as introduced by Vaswani et al. [275], utilizes a vanilla attention mechanism that computes dense pairwise relations in the input sequence. This results in quadratic complexity. However, recognizing that not all these relations hold equal significance, recent research has focused on methods to streamline this process. These methods aim to identify and maintain only the most crucial relations, thereby enhancing the efficiency of the attention calculation [7, 50, 58, 60, 68, 169, 171, 350]. In this subsection, we include the discussion of two main branches: (i) the use of fast or sparse attentions [50, 52, 57, 68, 137, 183, 186, 349] and *(ii)* IO-aware attention calculation with hardware co-design [58, 60, 63, 97, 98, 108]. Both approaches reduce hardware loading time for efficient attention calculations.

Fast Attention Calculation. In the realm of fast attention, researchers are developing innovative strategies to enhance efficiency. A primary focus is on *attention factorization*, which aims to reduce attention calculations that are often unnecessary in certain contexts. This technique is particularly useful when dealing with lengthy sequential inputs, where direct pairwise attention computations become computationally intensive. By employing attention factorization, computational demands can be significantly reduced, transforming 2-D computations into more manageable 1-D formats [8, 50, 183, 319]. Furthermore, these factorized attention methods are designed to discern and emphasize the attention differences between closely positioned tokens and their respective changes over time. This nuanced approach ensures that the computational resources are dedicated to the most impactful elements of the data. Another innovative method involves the use of *frequency-based techniques*, such as Fast Fourier Transform (FFT) and hash representations. These techniques model attention in a manner that aligns well with hardware capabilities, making them more efficient for practical applications [62, 261, 350]. They filter out near-zero attentions and focus computational efforts on the most significant ones for the final calculations. Such selective attention ensures that resources are not wasted on processing relatively unimportant data, further optimizing the overall efficiency of the model.

Moving away from directly calculating pairwise attention, some methods [59, 68, 80, 169] explore the possibility of computing attention at a *block level*, which enables parallelization of the computation, significantly enhancing efficiency.

For instance, the Monarch framework [59] and its advanced version, Monarch Mixer (M2) [80], adopt a novel strategy. They sparsify the dense attention matrix by decomposing it into a combination of permutation and block diagonal matrices. This decomposition allows for more efficient processing of attention calculations. Furthermore, the Blockwise Self-Attention (BST) method [169, 171] introduces blockwise computation for both self-attention and feed-forward networks. This technique aims to lower memory requirements typically associated with traditional attention mechanisms. Moreover, some methods like LongNet [68] incorporate dilated attention in place of the original dense attention, allowing the processing of much longer token sequences, thereby expanding the capabilities of LLMs. Hardware-Related Efficient Attention. Along with designing more efficient attention mechanisms at the software level, a significant focus has shifted to optimizing these mechanisms at a hardware level. One of the main challenges in this domain is efficiently utilizing computational resources, such as High Bandwidth Memory (HBM) and Static Random-Access Memory
(SRAM), on GPUs. In this regard, recent advancements like FlashAttention [60] and its successor, FlashAttention-2 [58],
have emerged. FlashAttention reconsiders the attention computation by adopting an I/O-centric perspective. It minimizes data transfers between HBM and SRAM, addressing a critical bottleneck in GPU processing. This method integrates blockwise softmax value calculations and updates with statistics. Such integration eliminates the conventional requirement of computing the softmax function only after all attention values have been determined. Building upon FlashAttention [60],
FlashAttention-2 [58] further optimizes work partitioning and reduces non-matrix multiplications, capitalizing on the GPUs' optimization for matrix operations. These algorithms are tailored to hardware considerations and accelerate the models on GPU machines. Based on FlashAttention, FlashDecoding [63] and FlashDecoding++ [108] split the keys/values in smaller chunks for parallelization of partial attention, and FlashDecoding++ [108] further introduce asynchronized softmax with unified max value and flat GEMM optimization with double buffering for further speed up, along with the heuristic dataflow for adaption on hardware resources.

In the quest to enhance Large Language Model (LLM) systems, some researchers are creatively drawing inspiration from current hardware architectures. A notable example is the PagedAttention [147], which adapts virtual memory and paging techniques, commonly used in operating systems, to overcome memory limitations in LLMs. PagedAttention introduces an innovative approach to memory management by emulating the virtual memory system. It segments the Key-Value (KV) cache associated with a request into blocks, as opposed to relying on pre-allocated, contiguous memory. This method significantly reduces memory fragmentation, a common issue in traditional LLM memory allocation strategies. As a result, it allows the LLM to process longer sequences within the constraints of limited memory resources.

## 5.3 Efficient Positional Encoding

Since LLMs may need to process long sequences as input, the absolute positional encoding (APE) used in the vanilla Transformer [275] falls short of this requirement. To enhance the architecture's efficiency, researchers are exploring novel positional encoding (PE) methods that can accommodate longer sequences with relative positions [47, 48, 162, 212] or rotary positional encoding [206, 256]. They are also seeking more generalizable solutions through randomized positional encoding [229] or even omitting positional encoding [129]. We discuss some of the latest developments in this section.

Addition-based Relative Positional Encoding. Relative positional encoding methods utilize the relative position between two tokens rather than the absolute position of a single token. Some of them encode the relative positions and add the encoded positions to the subsequent attention, referring to *addition-based* relative positional encoding methods. T5 [217], TISA [288],
and FIRE [162] are representatives of this paradigm. In these models, the position embedding is applied to the interaction between the query and key elements within the self-attention mechanism, a departure from the earlier focus on the absolute position of individual tokens. The relative positional encoding in T5 [217] translates the relative position difference into a scalar bias value using a lookup table and employs the same embedding for all out-of-distribution (OOD) sequence lengths.

TISA incorporates [288] a trainable Gaussian kernel that focuses on the positional differences between tokens. FIRE [162], on the other hand, employs progressive interpolation with a normalized position index by dividing the index difference between tokens by the smaller of the two indices. Compared to APE, relative positional encoding (RPE) offers a more effective way of modeling the relative distances between tokens. This not only enhances the model's understanding of token relationships but also facilitates length extrapolation, a critical feature for handling varied and complex sequences in language processing. Relative Positional Encoding with Decay Functions. Another trend is to employ trainable relative positional encodings
(RPE) that use decaying functions. This approach, exemplified by models like ALiBi [212], KERPLE [47], and Sandwich [48],
aims to focus the model's attention predominantly on neighboring tokens. The use of decaying functions in these methods ensures that the attention diminishes as the distance between tokens increases. ALiBi introduces a linear decaying function to model the relationship between tokens, particularly effective for capturing the diminishing relevance of tokens as their distance increases. KERPLE [47] uses two variations of conditionally positive definite (CPD) kernels: a logarithmic variant and a power variant. These sophisticated kernels decay the connection between two tokens during RPE computation to adaptively model the decreasing significance of distant token relationships. Sandwich [48], meanwhile, adopts a series of cosine functions to represent the differences between tokens. Sandwich leverages the periodic nature of cosine functions to capture the cyclical patterns in token relationships. By diminishing attention between distant positions, these methods ensure the model's focus remaining on the more immediate and contextually relevant tokens rather than the tokens that are far away.

Rotary Positional Encoding. Beyond addition-based relative positional encoding, which adds encoded positions to the attention calculation, there are RPE methods that utilize rotary matrices for position embeddings [34, 206, 256]. RoPE [256]
introduces two rotation matrices to rotate the query and key vectors. The rotation angle is proportional to their absolute positions, which is then integrated into the dot product attention mechanism. This manner allows RoPE to generate attention based on the relative distance between tokens, instead of directly computing their relative differences. However, RoPE faces limitations in generalizing to sequence lengths beyond what it was trained on. Building upon RoPE, PI [34] extends its capabilities with Position Interpolation (PI). After fine-tuning on a moderate amount of data, PI shows a promising ability to handle very long context windows, addressing one of RoPE's primary limitations. YaRN [206] further advances this field by introducing NTK-aware interpolation and dynamic NTK interpolation. This method effectively addresses the loss of high-frequency information in scenarios with and without fine-tuning on limited datasets. YaRN's approach significantly improves the model's ability to expand context size without the necessity for extensive fine-tuning. The common thread among these methods is their use of rotary matrices in the query and key vectors, a technique that has shown promising results in establishing more effective RPE in Transformer models.

Other Positional Encodings. Exploring beyond relative positional encoding (RPE) methods, Randomized PE [229] and NoPE [129] present approaches that do not rely on modeling the consecutive positions of tokens in the input query. Intriguingly, they posit that by including positions outside the length of the training distribution or by forgoing positional encodings altogether, the model can handle out-of-distribution cases with longer token lengths and exhibit enhanced generalizability on downstream tasks. Randomized PE [229] employs a number greater than the longest sequence encountered during training.

It randomly samples from a range of integers, using them as indices after sorting. This approach enables the model to generalize to longer sequences during inference, though it requires prior knowledge of the maximum token length. On the other hand, NoPE completely forgoes the positional encoder in the self-attention mechanism. It demonstrates that the model's self-attention can inherently learn the RPE across tokens in a sentence. This omission not only simplifies the model architecture but also shows promising results in terms of generalizability, especially for sentences with queries extending beyond the training distribution.

## 5.4 Sparse Modeling

In the quest to optimize Transformers for efficiency, another key area of research focuses on integrating sparse modeling within these attention-based architectures. This approach is pivotal in reducing computational demands, especially in models with a large number of parameters. Two primary directions have emerged in sparse modeling: the Mixture of Experts
(MoE) [43, 72, 153, 193, 219, 244, 352] and Sparsefinder [272] from different manners.

The MoE approach [43, 44, 72, 78, 248, 313], incorporates multiple branches or 'experts' in the model, each specializing in different subtasks. During inference, only a subset of these paths is activated, maintaining computational efficiency while potentially enhancing performance. This design enables models like GLaM to scale impressively, activating only 99 billion parameters during inference despite having over 1.2 trillion parameters in total. Further developments in MoE, such as Sparse MoE [43], address issues like representation collapse, ensuring more equal activation of experts and efficient information processing. On the other hand, Sparsefinder [272] takes a different approach by focusing on uncovering sparsity within the The Efficiency Spectrum of Large Language Models: An Algorithmic Survey Efficient LLM Algorithmic Survey, Nov, 2023, USA.

Table 1. Comparisons of time and memory cost in inference evaluation between Transformer and attention-free methods when using a sequence of length  as input. Models with more + under performance are of better performance of the perplexity numbers on the in-domain validation set and other out-of-domain corpora.

| Method            | Time Cost   | Memory Cost   | Performance   |
|-------------------|-------------|---------------|---------------|
| Transformer [275] | 𝑂(𝑛)      | 𝑂(𝑛 2 )     | +++(+)        |
| RWKV [204]        | 𝑂(1)       | 𝑂(𝑛)        | +             |
| H3 [61]           | 𝑂(1)       | 𝑂(𝑛 log𝑛)  | ++            |
| Hyena [210]       | 𝑂(𝑛)      | 𝑂(𝑛 log𝑛)  | +             |
| RetNet [260]      | 𝑂(1)       | 𝑂(𝑛)        | +++           |
| Mamba [89]        | 𝑂(1)       | 𝑂(𝑛)        | +++           |

attention mechanism itself. This method identifies key patterns through the attention scheme, which helps in efficiently allocating computational resources to the most impactful areas of the model.

## 5.5 Attention-Free

One significant drawback of the vanilla attention mechanism [275] is the quadratic complexity of attention computation, making it especially inefficient for handling long sequences. Although efficient / sparse attention offers some relief, its worst-case theoretical complexity remains unchanged. To address this, various attention-free methods have been proposed, providing alternatives that avoid the computation of the quadratic attention matrix [61, 91, 204, 210, 260, 324]. These methods could be largely categorized into those that replace the attention mechanism with recurrent computation [204, 260, 324], and those that discretize state space representations [89–91, 95, 187, 253]. Notably, these new methods like RWKV [204], H3 [61], Hyena [210], RetNet [260] and Mamba [89] achieve performance comparable to the standard Transformer. RWKV [204] utilizes recurrent neural networks (RNNs) to streamline sequence processing, thereby reducing the complexity of handling long sequences. H3 [61], based on state space models (SSMs), offers an efficient alternative for data representation and processing.

Hyena [210] presents itself as a drop-in replacement for traditional attention mechanisms, simplifying the Transformer architecture. RetNet [260] introduces a multi-scale retention module coupled with a feed-forward network module, enhancing parallelism and recurrence, which significantly improves efficiency in both training and inference phases. Mamba [89]
includes the selection operation based on state space models for as compression and further improve the efficiency with hardware-related optimization. We include the complexity analysis of these methods compared with the vanilla Transformer with an input query of length  in Table 1. It provides an overview of how each method scales in complexity, offering insights into the advancements in the attention-free technologies.

## 6 Training And Tuning Efficiency 6.1 Introduction

The development of training and tuning techniques for LLMs must address the challenges posed by the ever-increasing size of data and models. This section delves into the efficiency aspects crucial for both scalable training and tuning of LLMs, highlighting key areas of focus. Memory Efficiency. The rapid growth in the number of parameters in large transformer models, increasing by approximately 410× every two years, presents significant memory challenges. This growth has outpaced the expansion of GPU memory, which has seen only a 5× increase (from 16GB to 80GB) over the same period. The actual memory consumption during 15 training far exceeds the raw parameter count, encompassing model states (parameters, gradients, optimizer states), as well as residual states (intermediate activations, temporary buffers, memory fragmentation). Given these constraints, single GPU
setups are insufficient for handling entire models, necessitating distributed training approaches like tensor parallelism (TP) and pipeline parallelism (PP) for effective memory management. Computation Efficiency. While distributed training offers potential benefits in speeding up the training of large models, it also introduces complexities that affect scalability. A notable observation is the decrease in FLOPs per GPU when training is distributed across multiple GPUs, as opposed to a single GPU setup. This decrease stems from the challenges in efficiently utilizing an increasing number of computational resources. Therefore, scalability becomes a crucial element in boosting computation efficiency during the training process, particularly in multi-GPU settings.

Communication Efficiency. This aspect relates to the exchange of parameters and gradients between different devices or layers during training. Techniques like all-reduce are employed to synchronize gradients across all devices at the end of backward propagation in data parallel training. The goal is to minimize the volume of communication data during collective operations such as broadcast, reduce, all-reduce, and all-gather.

In short, training and tuning LLMs is a complex challenge that demands a comprehensive approach. An integrated strategy that considers all these efficiency aspects is vital for the effective and scalable training and tuning of LLMs. The subsequent sections will provide a detailed exploration of these aspects.

## 6.2 Scalable Training

6.2.1 Stable Training Strategies. During the pre-training of LLMs, ensuring training stability is a critical aspect of efficiency.

Training instability, often manifested as vanishing or exploding gradients, can significantly hinder the training process. To mitigate these issues, careful selection and adjustment of hyperparameters are essential. One effective approach involves the strategic manipulation of batch size. For example, models like PaLM [53] gradually increase their batch size from 1 million to 4 million tokens during training. This gradual scaling helps in accommodating the model's growing capacity to process larger data volumes without compromising stability. Another key hyperparameter is the learning rate, where the warm-up cosine scheduler is commonly employed. This scheduler initially increases the learning rate during the early stages of training (typically 0.1% to 0.5% of total training steps) and then implements a cosine decay strategy. This approach gradually reduces the learning rate to about 10% of its peak value, ensuring a balance between rapid learning and stability as training progresses. The choice of optimizer also plays a pivotal role in stabilizing the training of LLMs. Optimizers like Adam [135] and AdamW [179] are popular choices for models such as GPT-3 [23] and OPT [331], owing to their momentum feature that accelerates convergence by leveraging past gradient information. Additionally, the Adafactor [245] optimizer, known for its GPU memory efficiency, is utilized in models like PaLM and T5 [217]. Beyond hyperparameter tuning, implementing stabilizing strategies like weight decay and gradient clipping is common to prevent exploding gradients. However, even with these measures, training loss spikes can still occur, often influenced by both the current model state and the data being processed. To address this, models like PaLM and OPT employ a strategy of restarting the training from a previous checkpoint when a spike is detected, effectively skipping over the data that triggered the instability. This approach ensures not only training stability but also efficient use of computational resources by avoiding prolonged periods of unproductive training.

6.2.2 Mixed Precision Training. In the realm of LLM pre-training, mixed precision training emerges as a critical strategy for enhancing both memory and computational efficiency. Traditionally, neural network training involves storing weights, gradients, and activations in full-precision (FP32) format. However, for extremely large models, this approach can be resourceintensive. To address this, reduced-precision formats like FP16 or INT8 are adopted. These formats not only reduce memory usage but also expedite communication processes within the model. In addition, modern GPUs are typically more adept at handling FP16 computations compared to FP32, offering a further boost in computational speed.

Despite these advantages, transitioning directly from FP32 to FP16 can sometimes lead to performance degradation [116, 342]
due to issues like overflow or underflow inherent in FP16. To circumvent these challenges, the automatic mixed-precision
(AMP) [188] method has been developed. AMP maintains a master copy of weights in FP32 while employing FP16 for computations during the forward and backward passes. Post-calculation, the weights are converted back to FP32 for updating the master weights. This method, coupled with a loss scaling technique that preserves small gradient values, enables AMP
to match the accuracy of FP32 training without the need for extensive hyperparameter tuning. Further advancements in precision reduction have led to the introduction of Brain Floating Point (BF16) [125], a novel half-precision format. BF16, designed to cover the same range as FP32 by allocating more bits to the exponent and fewer to the significand compared to FP16, has demonstrated state-of-the-art performance and more reliability than mixed precision under FP16.

Another innovative approach is Activation Compressed Training (ACT) [32], which focuses on compressing activations to an average of 2 bits across multiple tasks. ACT computes gradients using a compressed version of activations saved during the backward process, leading to a significant reduction in memory requirements for activations. This compression enables training with substantially larger batch sizes, ranging from 6.6× to 14× larger than traditional methods. Overall, mixed precision training stands as a testament to the evolving landscape of LLM training, where efficiency and performance are continually balanced through innovative techniques.

6.2.3 Parallelism-Based Techniques. Parallelism in the training of LLMs is a strategy that involves distributing the computational workload across multiple accelerators, such as GPUs or TPUs. This approach is crucial for managing the substantial data and complex computations required in LLM training, facilitating the development of more advanced and capable models.

In this section, various parallel training schemas are discussed.

Data Parallelism (DP). Data parallelism [76, 160, 163, 300] a straightforward yet effective form of distributed training. In this approach, the dataset is divided into smaller subsets, which are then processed in parallel across multiple accelerators.

The model is replicated across these devices, with each replica operating on a separate unit. Each unit then independently performs forward and backward computations on its assigned subsets. A key aspect of DP is the synchronization of gradients at the end of each training step. The gradients calculated on each device are averaged, resulting in a gradient representative of the entire batch. This process ensures that despite the parallel processing, the model learns consistently across all subsets of data. DP is particularly noted for its ability to maximize GPU utilization. However, it requires high-bandwidth interconnects to efficiently handle the communication demands between devices. By leveraging DP, training large-scale LLMs becomes more feasible, enabling faster development cycles and the exploration of more complex model architectures.

Model Parallelism (MP). Model Parallelism is an alternative approach to DP, focusing on dividing the model itself across multiple accelerators. This method is particularly useful for handling models with a large number of parameters and sizes, especially when a single GPU lacks the capacity to store the entire model. Model Parallelism can be further categorized into two types: Tensor Parallelism (TP) and Pipeline Parallelism (PP).

- Tensor Parallelism [243, 250, 303] is a form of *intra-layer* model parallelism. It involves dividing the tensors of individual layers across multiple accelerators, allowing for the training of models that are larger than the memory capacity of a single

| Table 2. Summary of different parallelism strategies for efficiency.   |                                         |               |      |      |
|------------------------------------------------------------------------|-----------------------------------------|---------------|------|------|
| Parallelism Strategy                                                   | Resource Efficiency                     |               |      |      |
| Memory                                                                 | Computation                             | Communication |      |      |
| Data Parallelism (DP)                                                  | Low                                     | High          | High |      |
| Tensor Parallelism (TP) (Intra-layer)                                  | High                                    | Low           | Low  |      |
| Model Parallelism (MP)                                                 | Pipeline Parallelism (PP) (Inter-layer) | High          | Low  | High |

GPU. A notable example of this approach is Megatron-LM [250], which provides strategies for slicing different components of transformer parameters, such as MLP layers, self-attention layers, and output embedding layers. This slicing can be done either horizontally or vertically. Initially, TP focused on splitting two-dimensional matrices, but it has since evolved to include multi-dimensional splitting. Frameworks like Colossal-AI [20, 278, 302] have expanded TP to higher dimensions and introduced sequence parallelism [142, 161] for handling sequence data. While TP is efficient in terms of memory usage, it requires high interconnect bandwidth for effective layer communications.

- Pipeline Parallelism [114, 120, 133, 143, 195–197, 306], on the other hand, is a form of *inter-layer* model parallelism. It involves splitting the layers of a model across multiple accelerators in a pipeline configuration. Each accelerator is responsible for computing a different layer and then passing its output to the next, akin to an assembly line. This setup allows for sequential yet concurrent processing of both forward and backward passes. While one segment of the model processes one part of the data, other segments can work on different parts simultaneously. The key to the efficiency of PP lies in its ability to keep all parts of the model active and productive, though it requires careful scheduling to minimize idle time on the GPUs.

Gpipe [114], one of the earliest proposals for PP, combines PP with mini-batch splitting. It segments a large model across multiple GPUs and processes input mini-batches as smaller micro-batches. This approach allows for the efficient training of significantly large models. GPipe also uses a strategy called rematerialization [42] to reduce memory usage by recalculating activations during backward propagation instead of storing them. However, GPipe still faces memory inefficiencies due to the need to cache activations during backward computations. PipeDream [99, 195] further refines the PP approach with its One Forward pass followed by One Backward pass (1F1B) strategy. This method allows for the immediate backward propagation of a micro-batch following its forward pass, enabling earlier stages of the pipeline to start their backward computations sooner. PipeDream also employs asynchronous gradient updates using different versions of weights and optimizes memory allocation across the pipeline stages for more efficient processing. BPipe [133] introduces a technique to balance memory usage throughout the pipeline. It leverages idle memory in later stages to support earlier stages, significantly speeding up the training process for large models like GPT-3 96B and GPT-3 134B by 1.25 to 2.17 times compared to Megatron-LM. TeraPipe [?

] addresses the challenge of training large models with extensive sequence lengths, which can lead to smaller mini-batches and increased idle time in the pipeline. In transformer architectures, some layers' calculations don't depend on future hidden states. TeraPipe uses this fact to enable parallel processing by dividing the input sequence. It uses dynamic programming to effectively split the sequence at the best points across tokens, improving the efficiency of parallel processing.

Automated Parallism. Automated Parallelism has become a key approach in scaling up modern LLMs, combining various parallelism methods for optimal performance. Systems like DeepSpeed [221], Megatron-LM [250], and Colossal-AI [302] have adopted a 3D parallelism approach, which involves distributing training data uniformly across workers, manually partitioning the model, and distributing layers in each pipeline stage. However, this manual orchestration of parallelism types is complex and not easily adaptable across different models and computing environments.

To streamline this process, automated parallelization solutions are being developed. These solutions aim to speed up model deployment and ensure adaptability across different models. As models and computing clusters grow, the complexity of parallelism configurations also increases. Tofu [281] tackles this challenge with a dynamic programming algorithm that optimizes dataflow graph partitioning. Dapple [76] focuses on minimizing pipeline latency through optimal partitioning strategies. However, these solutions are currently limited to combining data parallelism with only one model parallelism type, due to the complex interactions between different parallelism strategies. Alpa [337] takes a more comprehensive approach by organizing data, model, and pipeline parallelism into a hierarchical structure. It uses integer linear programming for model parallelism plans and dynamic programming for pipeline parallelism plans. Alpa is comparable to specialized systems like Megatron-LM in training GPT models, demonstrating its effectiveness in handling complex parallelization challenges.

FlexFlow [120] extends the concept of 3D Parallelism by proposing a method to divide operation output tensors across different dimensions (Sample, Operation, Attribute, Parameter). Each operation in the computation graph is assigned a specific parallelization configuration. To find the best parallelization strategy, FlexFlow uses an execution simulator that predicts the time required to run an operator graph on a given device topology. It then employs Markov Chain Monte Carlo sampling to systematically search for the optimal strategy, considering both the operator graph and device topology.

6.2.4 Memory Optimization. In the realm of training LLMs with increasing sizes, the memory needed to store model parameters, gradients, and optimization states grows significantly. This challenge is particularly acute in DP, where each GPU traditionally stores a complete copy of the model's parameters, leading to considerable memory redundancy. Efficient memory optimization strategies are essential to train larger models on limited hardware resources, balancing the memory load across different components of the training infrastructure.

ZeRO [220] addresses the issue of memory redundancy in data parallelism by partitioning the memory load across GPUs.

Instead of each GPU storing the entire set of model parameters, gradients, and optimizer states, ZeRO divides these elements, allowing each GPU to hold only a portion of the data. The remaining data can be retrieved from other GPUs as needed. This approach includes three key strategies: parameter partitioning, gradient partitioning, and optimizer state partitioning, each targeting a specific aspect of the model's memory requirements. Building on ZeRO, ZeRO offload [223] extends these concepts to enable the training with the usage of both CPU and GPU capabilities, offloading some computations and storage to the CPU to alleviate the memory burden on GPUs. However, this offloading increases communication between the CPU and GPU,
which can become a bottleneck if not managed carefully. The strategy involves viewing the training process as a data flow graph, with different computation nodes assigned to different devices. The forward and backward processes are handled by the GPU, while parameter updates and precision conversions are managed by the CPU. This approach aims to minimize CPU computation and reduce communication overhead, ensuring efficient use of CPU and GPU resources in the training process.

Integrating these advancements, systems like DeepSpeed [221] offer different levels of memory optimization. The first stage is ZeRO-DP (Data Parallelism) which optimizes memory by partitioning only the optimizer states across GPUs. The second stage is ZeRO-R (Reduction and Partitioning) to further reduce memory usage by partitioning gradients and optimizer states. The third stage is ZeRO-Infinity that extends the memory optimization beyond what is available on the GPU, utilizing both CPU and NVMe memory to enable training of extremely large models.

## 6.3 Scalable Tuning

Large Language Models trained on massive and varied datasets have demonstrated remarkable general problem-solving capabilities. However, their performance can be significantly enhanced for specific domains or tasks through targeted adaptation. In recent years, a range of techniques has emerged to facilitate this adaptation process. This section discusses two primary approaches for the efficient adaptation of pretrained LLMs: (i) parameter-efficient fine-tuning, which involves incorporating adapter layers or fine-tuning existing parameters of the pretrained models, and *(ii)* the integration of taskspecific context via prompt engineering. These methods represent key strategies in tailoring LLMs to specific applications, ensuring both their versatility and effectiveness in diverse NLP tasks.

6.3.1 Parameter-Efficient Fine-Tuning (PEFT). The substantial size of pretrained LLMs makes them expensive or impractical to fully fine-tune the entire models for the downstream task or application domain. To avoid directly fine-tuning the full LLMs, a range of parameter-efficient tuning methods have a variety of parameter-efficient tuning methods have emerged.

These methods focus on refining LLMs by adjusting or introducing a small number of trainable parameters, while keeping most or all of the original pretrained parameters fixed. Such methods typically attain commendable performance and bring significant reductions in the quantity of trainable parameters. They enhance both memory and computational efficiency in comparison to full parameter tuning, offering more practical solutions for adapting LLMs to specific tasks. Partial Parameter Tuning. A straightforward yet effective approach in adapting LLMs is partial parameter tuning, where only a selected fraction of pretrained parameters are fine-tuned, leaving the rest unchanged. This method has been widely demonstrated. For example, the works [144, 150] fine-tune only a few final layers, achieving up to 90% of the performance of a fully fine-tuned model. Xie et al. [297] involves selecting a subset of layers for fine-tuning based on the variability in their hidden states, particularly for classification tasks. Additionally, BitFit [320] presents an alternative strategy by adjusting only the bias terms in transformer-based LLMs, yielding competitive performance. These examples underscore the potential of partial parameter tuning as a resource-efficient way to adapt LLMs for various applications. However, these methods typically lack detailed principle to guide how to select a subset of parameters for further tuning.

Model-Adapter tuning. To tackle the issue of selecting specific parameters for fine-tuning, the technique of adapter tuning has been introduced, which involves augmenting the pre-trained model with additional small-scale learnable blocks, known as adapters [109]. Such approaches maintain the integrity of the pre-trained model, yet embed adapter blocks into one or several modules of the pretrained LLMs. These adaptors typically take the form of compact bottleneck layers. One example is comprising a two-layer MLP (Multi-Layer Perceptron) with a nonlinearity function and a small number of neurons in the hidden layer. The adaptor integration can be executed in series [109] or in parallel [209] with the attention and feed-forward layers of the Transformer architecture, or outside of the Transformer architecture [205]. To further enhance the reuse and versatility of adapters, AdapterHub [208] has been developed. This framework allows for the dynamic integration of pre-trained adapters, catering to a variety of tasks and LLMs. Although the use of adapters accelerate the fine-tuning process and mitigates storage requirements, its does modify the computational graph by adding depth or width to each transformer layer. Such modification results in a slight increase in inference latency, as observed in studies [228], where inference speeds were found to be slower by approximately 4-6%.

Parameter-Adapter tuning. Another related approach is to directly add an adapter to the model parameters. Denoting the pre-trained network parameters as , this class of techniques expands the model parameters to  + Δ, with  being fixed and Δ being learned by low-rank approximations. An implementation of this technique is diff-pruning [94] that learns task-specific sparse parameters Δ by adding a sparse promoting regularization during fine-tuning. The method LoRA [111] learns low-rank transformations for each linear layer. In particular, LoRA reparameterize the weight matrix as  +Δ ≈  +,
where the pretrained weight matrix  is fixed, yet the low-rank matrices  and  are learnable. In LoRA, all weight matrices share a constant intrinsic rank for each low-rank sub-matrix, while not accounting for the varying importance across different modules. AdaLoRA [328] addresses this limitation by dynamically allocating parameter budgets to weight matrices based on their importance scores. It assigns higher ranks to more critical incremental matrices, capturing more detailed task-specific information, while reducing the rank of less important matrices to avoid overfitting and save computational resources.

SoRA[69] introduces an optimize-able gate that dynamically adjusts the rank of the incremental matrix using the proximal gradient method. In QLoRA [66], the pretrained model is initially quantized to 4 bits. Subsequently, a small set of learnable low-rank adapter weights are augmented and fine-tuned using backpropagated gradients over the quantized weights. QLoRA can match the performance of 16-bit full-parameter fine-tuning even with 16-bit, 8-bit, or 4-bit adapters.

6.3.2 Data-Efficient Tuning. Data-efficient tuning refers to the process of updating a limited set of prompt parameters for downstream tasks, instead of fine-tuning the pretrained LLM. It is typically achieved through prompt tuning, where the weights of a pretrained model are kept fixed, yet only the added prompt tokens are adjusted. Such approaches enable a more efficient use of data and often yield enhanced performance, particularly as the scale of the model parameters increases. Prompt Tuning. Prompt tuning is a technique used to enhance the performance of LLMs in supervised downstream tasks. It formulates the downstream task into a masked language problem and converts the original token input into a template and masking certain tokens unfilled for the LLMs to complete. By modifying the tunable template embedding, prompt tuning aims to improving performance in the downstream tasks via reducing the distribution shift between the pretrained tasks and the specified downstream tasks. This method also enables the LLM to engage in few-shot or even zero-shot learning, especially useful in scenarios with limited supervised data, by generating new prompt templates.

Traditional methods required manual design of prompt templates and verbalizers, which often resulted in sensitive and varying efficacy. However, recent advancements in prompt learning have led to the automation and optimization of prompt construction. AutoPrompt [249] introduces a gradient-based approach to automate the search for effective templates. LMBFF [83] offers a more efficient solution for automated prompt generation by searching for label words and using a T5-based template generation method in the discrete prompt space. To tackle the challenges of discrete optimization, Prefix-Tuning [165]
recommends parameterized prompts, where only the prompt is fine-tuned while the LLM remains unaltered. P-tuning [174] breaks away from the conventional constraint that templates must be composed of natural language, transforming template construction into a continuous parameter optimization challenge. CP-tuning [304] advocates the use of contrastive learning to automatically learn the distribution of embeddings, serving as a substitute for manual verbalizer design. UPT [280] introduces the Prompt-Options-Verbalizer paradigm, facilitating joint prompt learning across diverse NLP tasks and encouraging LLMs to acquire task-invariant prompting knowledge.

## 7 Inference Efficiency 7.1 Introduction

The enormous number of parameters in Large Language Models (LLMs) poses significant challenges for deployment on cloud services and resource-limited devices, leading to high maintenance costs for inference support. Consequently, accelerating inference has become a pressing issue garnering attention from both industry and academia. One common way is to construct compact model that could reach competitive performance to the full model, which methods can be broadly classified into four categories: pruning, knowledge distillation, quantization, and low-rank decomposition. Pruning techniques focus on identifying and eliminating redundancy within the operators of Deep Neural Networks (DNNs), thereby creating more streamlined versions. Knowledge distillation involves transferring insights from larger, more complex 'teacher' models to smaller, more efficient 'student' models, helping to maintain high performance even in reduced models. Quantization reduces computational load and storage requirements by representing floating-point numbers in LLMs with fewer bits.

Low-rank decomposition approximates the heavy weight matrices in LLMs through low-rank matrices, further economizing computational resources. It's worth noting that some of these approaches require specialized computing libraries and hardware to achieve realistic resource savings and speed-up.

## 7.2 Pruning

Pruning techniques aim at identifying the redundancy inside operators of LLMs. Existing pruning techniques can be broadly classified as unstructured, semi-structured, and structured pruning. Unstructured Pruning. Unstructured pruning yields fine-grained sparsity wherein zero elements are randomly distributed across the trainable parameters [35, 38, 70, 71, 117, 146, 234, 242, 259, 294, 333]. These unstructured pruning methods show that LLMs can be pruned to at least 50% sparsity in one-shot, with(out) retraining, at minimal loss of accuracy. Meanwhile, larger LLMs are more compressible in the manner of unstructured pruning. Although unstructured pruning could bring theoretical inference speedup, the speedup is not easily reflected in reality due to the contiguity issue during sparse operations.

Accelerating a DNN with high fine-grained sparsity typically requires the supports of specialized designed software and hardware. There exist computing libraries such as FSCNN [119] which could outperform standard DNN runtime under sufficiently high unstructured sparsity, yet largely not extended to transformer architectures.

Semi-Structured Pruning. To mitigate the issue of unstructured pruning, semi-structured pruning is recently proposed where N:M sparsity is exampler [339]. N:M sparsity stays between unstructured pruning and structured pruning that every contiguous elements contain exactly  non-zero elements. Nvidia [51] introduced the Ampere Tensor Core GPU architecture
(e.g. A100 GPUs) and proposed the 2:4 fine-grained structured sparsity scheme that enables Sparse Neural Network to be accelerated on this hardware at inference time. This scheme places a constraint on the allowed sparsity pattern: For every contiguous array of four weights, two are pruned, yielding a 50%-sparse net. The resulting regular structure of the weight matrix allows one to compress it efficiently and to reduce memory storage and bandwidth by operating on the nonzero weights only. Importantly, Nvidia currently considered exclusively the 2:4 ratio; other are not accelerated yet.

Structured Pruning. Structured pruning removes entire neurons, channels, or other meaningful structures, thus preserving the functionality of remaining DNN that are amenable to efficient computation [37, 41]. Prior structured pruning methods require manual interventions to figure out removal structures, which is inconvenient. Recent works such as OTO [37, 39, 40] and torch-pruning [77] propose dependency graph analysis to automatically find out the structures that could be removed.

However, their application onto LLMs is facing significant challenges, due to the requirements of massive computational resources and the unavailable training datasets of both pretraining and instructed fine-tuning datasets [23]. Consequently, the paradigms of structured pruning on LLMs could be largely categorized as pruning under limited or *full* resources. For the limited-resource setup, LLM-Pruner [181] provides modular importance score calculator to rank removal structures.

The importance score is typically computed upon full gradient calculation which may be resource-consuming if with full models. A rapid post-training phase with limited instructed fine-tuning data is followed to recover lost knowledge to some extent. LoRAPrune [327] uses Low-Rank-Adaptor (LoRA) [111] during the pruning stage to reduce the resource requirements, yet still face significant performance degradation to the full LLMs. To recover and preserve knowledge more effectively, LoRAShear [36] is recently proposed in the limited resource setup. LoRAShear utilizes a novel structure sparse optimizer called LoRA Half-Space Projected Gradient (LHSPG) to conduct progressive structured pruning and transfer the knowledge. Unlike the prior works only using instructed-fine-tuning data, a multi-stage knowledge recovery mechanism is applied for LoRAShear to effectively narrow down the performance gap between the full and compressed LLMs. For full-resource setups, Sheared-LLaMA [295] performs structured pruning on original LLMs to create compact models that outperform equally sized LLMs trained from scratch. However, it requires significant GPU power and data resources, which may not be feasible for the public users. Compared with the relatively mature domain of structurally pruning middle-small size DNNs, structured pruning on LLMs is still in the early stage and awaiting for further explorations.

## 7.3 Knowledge Distillation

The concept of knowledge distillation involves utilizing supervisory signals from a large, more capable 'teacher' model to train a compact 'student' model. This approach often results in the student model surpassing the performance of a similarly sized model that was trained without such guidance [105]. Knowledge distillation can be largely categorized into response-based, feature-based, and relation-based knowledge distillation [88]. Response-based knowledge focuses on the final output layer of the teacher model. The hypothesis is that the student model will learn to mimic the predictions of the teacher model [14, 105].

A trained teacher model also captures feature-based knowledge of the data in its intermediate layers, which is especially pertinent for deep neural networks [225, 318]. Knowledge that captures the relationship between feature maps can also be used to train a student model, referring as relation-based [314]. Initial research in NLP domain primarily concentrated on the distillation of task-specific models [134]. Later on, more studies have shifted their focus towards distilling pre-trained models, which can subsequently be fine-tuned for specialized downstream tasks [123, 173, 233]. Recently, there emerge distillation methods for LLMs [11, 30, 46, 92, 110, 122, 159, 230, 282, 292, 325]. One main focus of the current knowledge distillation methods on LLMs lies in how to generate and utilize challenging (instructed) samples [46, 122, 292] to more effectively transfer the knowledge from teacher to student model, which has some overlapping to the data undersampling methods in Section 4.

Chain-of-thought prompting is commonly used in distillation approaches [110, 159] to accomplish the data generations.

However, current knowledge distillation methods for LLMs suffers from the absence of a standardized objective function, which is typically task specific and requires tuning efforts. Furthermore, the recent strategy of employing student-generated outputs to mitigate the discrepancies between training and inference has led to a substantial increase in computational costs.

Addressing these challenges, DistiLLM [138] has been recently introduced, consisting of two key components. (i) A skew Kullback-Leibler (KL) divergence loss with moderate skew value, which offers stable gradients and lower approximation errors compared to other alternatives, including the generalized Jensen–Shannon divergence. *(ii)* An adaptive off-policy technique, designed to improve the efficiency and effectiveness of using student-generated outputs.

## 7.4 Quantization

Quantization methods can be divided based on the necessity for retraining [86]. Quantization-Aware Training (QAT) mandates model retraining, adjusting its weights to recover accuracy post-quantization [16, 131, 247, 332]. In contrast, Post-Training Quantization (PTQ) achieves quantization without any retraining [26, 164, 199, 312, 317]. Although QAT often yields superior accuracy, it is frequently impractical for Large Language Models (LLMs) due to the prohibitive costs of retraining and, usually, the lack of access to original training data and processing infrastructure. As a result, most research on LLM quantization gravitates towards PTQ techniques.

From another perspective, quantization methods can be broadly classified into uniform and non-uniform approaches [86].

Uniform quantization, as explored in works like SPQR [67], GPTQ [79], and others [115, 131], involves dividing the range of weights into equally sized bins. This method has become popular for its ability to accelerate computation by allowing arithmetic operations in quantized precision rather than full precision. Additionally, uniform quantization may not be optimal in cases where the weight distribution is non-uniform, as often observed in LLMs. Contrarily, non-uniform quantization offers a solution to these challenges. As studied in SqueezeLLM [132], this approach allocates quantization bins non-uniformly, allowing for more flexibility and potentially better performance, especially when dealing with non-uniform weight distributions.

Compared with structured pruning, quantization requires specified hardware to realize the realistic advantage of low-bit precision to reduce the memory cost and inference speedup. For the LLM, due to lack of training data or the computing resources, structured pruning is typically difficult to effectively recover lost knowledge under high compression ratio. However, quantization could typically preserve the performance of LLM effectivelly. Therefore, quantization at the present is more popular and mature used in the LLM compression. It is in sharp contrast to the middle-small model size scenarios, where structured pruning and quantization are both commonly (jointly) used [37, 307].

## 7.5 Low-Rank Decomposition

The weight matrices in a DNN are often low-rank, indicating redundancy in model weights [231, 341, 347]. Thus, a natural idea is to factorize the weight matrices into two or more smaller matrices to save parameters. In LLMs, the weight matrices exist in linear layers including self-attention layers and MLP layers, and the embedding layers. There exists studies to factorize these weight matrices for saving parameter quantity and accelerate inference.

Decomposition on Linear Layer. Multi-linear attention [182] uses block-term tensor (BTT) decomposition [64] to factorize multi-head attention. Singular value decomposition (SVD) [64] is also commonly used and typically performed with a twostage manner. The first stage is to establish the decomposition followed by a second stage to fine-tune the low-rank weights via knowledge distillation [198]. Besides, as an alternative to BTT and SVD, Kronecker decomposition retains the rank of the matrix and has shown improvement during compressing BERT and GPT-2 [73, 262].

Decomposition on Embedding Layer. ALBERT [148] uses factorization for the embedding layer, which is one of the largest consumers of model parameters. Since the power of Transformer mainly comes from its contextual learning ability, the parameters in the token embedding layer are not efficient. It intuitively makes sense to reduce them by factorizing the embedding matrix. Self-Attentive Factorized embeddings (SAFE) [222] studied ways to share weights in transformers by adding a small self-attention layer on the basis of linear projection to achieve better performance than the alternatives. LightFormer [180] more effectively utilizes the parameter knowledge of the well-trained Transformer, and accelerates the convergence of the model factorization on the embedding layers.

## 8 Conclusion

In conclusion, the evolution of Large Language Models (LLMs) marks a significant milestone in the field of artificial general intelligence, bringing transformative changes across various domains. However, the rapid expansion of these models brings forth substantial challenges in terms of computational demands and memory requirements, creating hurdles for both academic research and practical deployment. This survey provided a comprehensive overview of the algorithmic innovations aimed at The Efficiency Spectrum of Large Language Models: An Algorithmic Survey Efficient LLM Algorithmic Survey, Nov, 2023, USA.

enhancing the efficiency of LLMs, capturing research developments mostly up to September 2023. Moving beyond the scope of the existing surveys that often focus on isolating aspects such as training or model compression, this survey delved into the multiple dimensions of efficiency that are crucial for the holistic algorithmic development of LLMs. It has spanned a broad array of efficiency-related topics including scaling laws, data utilization, architectural designs, as well as training, tuning, and inference strategies. The insights and analyses presented here aim to serve as valuable summarization for both researchers and practitioners in the field. By laying a solid foundation of current knowledge and approaches, this paper sets the stage for future breakthroughs and continued innovation in the crucial research area of LLM efficiency.

## References

[1] 2022. Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance. https://blog.research.google/2022/04/pathwayslanguage-model-palm-scaling-to.html
[2] 2023. Introducing Claude 2.1. (2023). https://www.anthropic.com/index/claude-2-1
[3] 2023. Introducing PyTorch Fully Sharded Data Parallel (FSDP) API. https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/ [4] 2023. Planning for AGI and beyond. (2023). https://openai.com/blog/planning-for-agi-and-beyond
[5] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and Ramachandran Ramjee. 2023. SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills. *arXiv preprint arXiv:2308.16369* (2023).

[6] Nur Ahmed and Muntasir Wahed. 2020. The De-democratization of AI: Deep learning and the compute divide in artificial intelligence research. arXiv preprint arXiv:2010.15581 (2020).

[7] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. *arXiv preprint arXiv:2305.13245* (2023).

[8] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang.

2020. ETC: Encoding long and structured inputs in transformers. *arXiv preprint arXiv:2004.08483* (2020).

[9] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. *Advances in Neural* Information Processing Systems 35 (2022), 22300–22312.

[10] Shun-ichi Amari, Naotake Fujita, and Shigeru Shinomoto. 1992. Four types of learning curves. *Neural Computation* 4, 4 (1992), 605–618.

[11] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. 2023. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. *GitHub* (2023).

[12] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. *arXiv preprint arXiv:2305.10403* (2023).

[13] Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. 2019. Deep batch active learning by diverse, uncertain gradient lower bounds. *arXiv preprint arXiv:1906.03671* (2019).

[14] Jimmy Ba and Rich Caruana. 2014. Do deep nets really need to be deep? *NeurIPS* 27 (2014).

[15] Lalit R Bahl, Peter F Brown, Peter V de Souza, and Robert L Mercer. 1989. A tree-based statistical language model for natural language speech recognition.

TASSP 37, 7 (1989), 1001–1008.

[16] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. 2020. Binarybert: Pushing the limit of bert quantization. *arXiv preprint arXiv:2012.15701* (2020).

[17] N Barkai, Hyunjune Sebastian Seung, and Haim Sompolinsky. 1993. Scaling laws in learning of classification tasks. *Physical review letters* 70, 20 (1993),
3167.

[18] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In *ICML*. 41–48. [19] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. 2023. Graph of Thoughts: Solving Elaborate Problems with Large Language Models. *arXiv preprint arXiv:2308.09687* (2023).

[20] Zhengda Bian, Qifan Xu, Boxiang Wang, and Yang You. 2021. Maximizing parallelism in distributed training for huge neural networks. *arXiv preprint* arXiv:2105.14450 (2021).

[21] Zalán Bodó, Zsolt Minier, and Lehel Csató. 2011. Active learning with clustering. In Active Learning and Experimental Design workshop In conjunction with AISTATS 2010. JMLR Workshop and Conference Proceedings, 127–139.

[22] Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference.

arXiv preprint arXiv:1508.05326 (2015).

[23] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. *NeurIPS* 33 (2020), 1877–1901.

[24] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. *arXiv preprint arXiv:2303.12712* (2023).

[25] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. 2022. Broken Neural Scaling Laws. In *The Eleventh International Conference on Learning* Representations.

[26] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Zeroq: A novel zero shot quantization framework. In CVPR. 13169–13178.

[27] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. *arXiv preprint arXiv:2307.03109* (2023).

[28] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. 2009. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. TNN 20, 3 (2009),
542–542.

[29] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. *arXiv preprint arXiv:1312.3005* (2013).

[30] Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, and Ji Zhang. 2023. MCC-KD: Multi-CoT Consistent Knowledge Distillation. *arXiv* preprint arXiv:2310.14747 (2023).

[31] Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo Zhao. 2023. Maybe Only 0.5% Data is Needed: A
Preliminary Exploration of Low Training Data Instruction Tuning. *arXiv preprint arXiv:2305.09246* (2023).

[32] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael Mahoney, and Joseph Gonzalez. 2021. Actnn: Reducing training memory footprint via 2-bit activation compressed training. In *International Conference on Machine Learning*. PMLR, 1803–1813.

[33] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374* (2021).

[34] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending context window of large language models via positional interpolation.

arXiv preprint arXiv:2306.15595 (2023).

[35] Tianyi Chen, Tianyu Ding, Bo Ji, Guanyi Wang, Yixin Shi, Jing Tian, Sheng Yi, Xiao Tu, and Zhihui Zhu. 2020. Orthant Based Proximal Stochastic Gradient Method for l1-Regularized Optimization. In *ECML PKDD*. 57–73.

[36] Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, and Luming Liang. 2023. LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery. *arXiv preprint arXiv:2310.18356* (2023).

[37] Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, Sheng Yi, and Xiao Tu. 2021. Only train once: A one-shot neural network training and pruning framework. *Advances in Neural Information Processing Systems* (2021), 19637–19651.

[38] Tianyi Chen, Bo Ji, Yixin Shi, Tianyu Ding, Biyi Fang, Sheng Yi, and Xiao Tu. 2020. Neural network compression via sparse optimization. arXiv preprint arXiv:2011.04868 (2020).

[39] Tianyi Chen, Luming Liang, Tianyu Ding, and Ilya Zharkov. 2023. Towards Automatic Neural Architecture Search within General Super-Networks. *arXiv* preprint arXiv:2305.18030 (2023).

[40] Tianyi Chen, Luming Liang, Tianyu Ding, Zhihui Zhu, and Ilya Zharkov. 2023. OTOv2: Automatic, Generic, User-Friendly. In The Eleventh International Conference on Learning Representations.

[41] Tianyi Chen, Guanyi Wang, Tianyu Ding, Bo Ji, Sheng Yi, and Zhihui Zhu. 2020. Half-space proximal stochastic gradient method for group-sparsity regularized problem. *arXiv preprint arXiv:2009.12078* (2020).

[42] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. *arXiv preprint arXiv:1604.06174* (2016). [43] Tianlong Chen, Zhenyu Zhang, Ajay Jaiswal, Shiwei Liu, and Zhangyang Wang. 2023. Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers. *arXiv preprint arXiv:2303.01610* (2023).

[44] Xin Chen, Hengheng Zhang, Xiaotao Gu, Kaifeng Bi, Lingxi Xie, and Qi Tian. 2023. Pipeline MoE: A Flexible MoE Implementation with Pipeline Parallelism.

arXiv preprint arXiv:2304.11414 (2023).

[45] Yuzhong Chen, Zhenxiang Xiao, Lin Zhao, Lu Zhang, Haixing Dai, David Weizhong Liu, Zihao Wu, Changhe Li, Tuo Zhang, Changying Li, et al. 2022.

Mask-guided vision transformer (mg-vit) for few-shot learning. *arXiv preprint arXiv:2205.09995* (2022).

[46] Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, and Kyle Richardson. 2023. DISCO: distilling counterfactuals with large language models.

In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*. 5514–5528.

[47] Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky. 2022. Kerple: Kernelized relative positional embedding for length extrapolation.

NeurIPS 35 (2022), 8386–8399.

[48] Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. 2023. Dissecting transformer length extrapolation via the lens of receptive field analysis. In ACL. 13522–13537.

[49] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.

[50] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*
(2019).

[51] Jack Choquette, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and Ronny Krashinsky. 2021. NVIDIA A100 tensor core GPU: Performance and innovation.

IEEE Micro 41, 2 (2021), 29–35.

The Efficiency Spectrum of Large Language Models: An Algorithmic Survey Efficient LLM Algorithmic Survey, Nov, 2023, USA.

[52] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2021. Rethinking attention with performers. *ICLR* (2021).

[53] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. *arXiv preprint arXiv:2204.02311* (2022).

[54] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences.

NeurIPS 30 (2017).

[55] John Joon Young Chung, Ece Kamar, and Saleema Amershi. 2023. Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions. *arXiv preprint arXiv:2306.04140* (2023).

[56] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023.

InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. *ArXiv* abs/2305.06500 (2023).

[57] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. *arXiv preprint arXiv:1901.02860* (2019).

[58] Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. *arXiv preprint arXiv:2307.08691* (2023).

[59] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher Ré. 2022.

Monarch: Expressive structured matrices for efficient and accurate training. In *ICML*. 4690–4721.

[60] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.

NeurIPS 35 (2022), 16344–16359.

[61] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré. 2022. Hungry hungry hippos: Towards language modeling with state space models. *arXiv preprint arXiv:2212.14052* (2022).

[62] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Ré. 2019. Learning fast algorithms for linear transforms using butterfly factorizations.

In *ICML*. PMLR, 1517–1527.

[63] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. 2023. *Flash-Decoding for Long-Context Inference*. https://princeton-nlp.github.io/flash-decoding/
[64] Lieven De Lathauwer. 2008. Decompositions of a higher-order tensor in block terms—Part II: Definitions and uniqueness. *SIAM J. Matrix Anal. Appl.* 30, 3
(2008), 1033–1066.

[65] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. *NeurIPS* 35
(2022), 30318–30332.

[66] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*
(2023).

[67] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. 2023. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression. *arXiv preprint arXiv:2306.03078* (2023).

[68] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. 2023. Longnet: Scaling transformers to 1,000,000,000 tokens. *arXiv preprint arXiv:2307.02486* (2023).

[69] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. 2023. Sparse Low-rank Adaptation of Pre-trained Language Models. *arXiv preprint arXiv:2311.11696* (2023).

[70] Tianyu Ding, Luming Liang, Zhihui Zhu, Tianyi Chen, and Ilya Zharkov. 2022. Sparsity-guided Network Design for Frame Interpolation. arXiv preprint arXiv:2209.04551 (2022).

[71] Tianyu Ding, Luming Liang, Zhihui Zhu, and Ilya Zharkov. 2021. Cdfi: Compression-driven network design for frame interpolation. In *Proceedings of the* IEEE/CVF conference on computer vision and pattern recognition. 8001–8011.

[72] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al.

2022. Glam: Efficient scaling of language models with mixture-of-experts. In *ICML*. PMLR, 5547–5569.

[73] Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. 2021. Kronecker decomposition for gpt compression. *arXiv preprint arXiv:2110.08152* (2021).

[74] Ronen Eldan and Yuanzhi Li. 2023. TinyStories: How Small Can Language Models Be and Still Speak Coherent English? *arXiv preprint arXiv:2305.07759*
(2023).

[75] Jeffrey L Elman. 1993. Learning and development in neural networks: The importance of starting small. *Cognition* 48, 1 (1993), 71–99.

[76] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long, Jun Yang, Lixue Xia, et al. 2021. DAPPLE: A Pipelined Data Parallel Approach for Training Large Models. (2021).

[77] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. 2023. Depgraph: Towards any structural pruning. *arXiv preprint arXiv:2301.12900*
(2023).

[78] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research 23, 1 (2022), 5232–5270.

[79] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers.

arXiv preprint arXiv:2210.17323 (2022).

[80] Daniel Y Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W Thomas, Benjamin Spector, Michael Poli, Atri Rudra, and Christopher Ré. 2023. Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture. In *NeurIPS*.

[81] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017. Deep bayesian active learning with image data. In *ICML*. PMLR, 1183–1192.

[82] William A Gale, Kenneth W Church, and David Yarowsky. 1992. A method for disambiguating word senses in a large corpus. *Computers and the Humanities* 26 (1992), 415–439.

[83] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. *arXiv preprint arXiv:2012.15723* (2020). [84] Jonas Geiping and Tom Goldstein. 2023. Cramming: Training a Language Model on a single GPU in one day.. In *ICML*. PMLR, 11117–11143.

[85] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. *TACL* 9 (2021), 346–361.

[86] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2022. A survey of quantization methods for efficient neural network inference. In *Low-Power Computer Vision*. Chapman and Hall/CRC, 291–326.

[87] Daniel Gissin and Shai Shalev-Shwartz. 2019. Discriminative active learning. *arXiv preprint arXiv:1907.06347* (2019). [88] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey. *IJCV* 129 (2021), 1789–1819.

[89] Albert Gu and Tri Dao. 2023. Mamba: Linear-Time Sequence Modeling with Selective State Spaces. *arXiv preprint arXiv:2312.00752* (2023).

[90] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Ré. 2022. On the parameterization and initialization of diagonal state space models. *NeurIPS* 35
(2022), 35971–35983.

[91] Albert Gu, Karan Goel, and Christopher Ré. 2021. Efficiently modeling long sequences with structured state spaces. *arXiv preprint arXiv:2111.00396* (2021). [92] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Knowledge Distillation of Large Language Models. *arXiv preprint arXiv:2306.08543* (2023).

[93] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks Are All You Need. *arXiv preprint arXiv:2306.11644* (2023).

[94] Demi Guo, Alexander M Rush, and Yoon Kim. 2021. Parameter-Efficient Transfer Learning with Diff Pruning. In *ACL-IJCNLP*. 4884–4896.

[95] Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. Diagonal state spaces are as effective as structured state spaces. *NeurIPS* 35 (2022), 22982–22994.

[96] Géza Györgyi and Naftali Tishby. 1990. Statistical theory of learning a rule. *Neural networks and spin glasses* (1990), 3–36.

[97] Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H Oh, Yeonhong Park, Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W Lee, et al. 2020.

Aˆ 3: Accelerating attention mechanisms in neural networks with approximation. In *HPCA*. 328–341.

[98] Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soosung Kim, Hyunji Choi, Sung Jun Jung, and Jae W Lee. 2021. ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks. In *ISCA*. 692–705.

[99] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. 2018. Pipedream: Fast and efficient pipeline parallel dnn training. *arXiv preprint arXiv:1806.03377* (2018).

[100] Michael Hassid, Hao Peng, Daniel Rotem, Jungo Kasai, Ivan Montero, Noah A Smith, and Roy Schwartz. 2022. How much does attention actually attend?

Questioning the Importance of Attention in Pretrained Transformers. *ACL Findings* (2022).

[101] Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. *arXiv preprint arXiv:2302.09210* (2023).

[102] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al.

2020. Scaling laws for autoregressive generative modeling. *arXiv preprint arXiv:2010.14701* (2020).

[103] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021. Scaling laws for transfer. *arXiv preprint arXiv:2102.01293* (2021).

[104] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou.

2017. Deep learning scaling is predictable, empirically. *arXiv preprint arXiv:1712.00409* (2017).

[105] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. *arXiv preprint arXiv:1503.02531* (2015). [106] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. *Neural computation* 9, 8 (1997), 1735–1780.

[107] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. *arXiv preprint arXiv:2203.15556* (2022).

[108] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong, and Yu Wang. 2023. FlashDecoding++: Faster Large Language Model Inference on GPUs. *arXiv preprint arXiv:2311.01282* (2023).

[109] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

Parameter-efficient transfer learning for NLP. In *ICML*. PMLR, 2790–2799.

[110] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.

Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. *arXiv preprint arXiv:2305.02301* (2023).

[111] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. In *ICLR*.

[112] Rong Hu, Brian Mac Namee, and Sarah Jane Delany. 2010. Off to a good start: Using clustering to select the initial training set in active learning. (2010).

[113] Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. *arXiv preprint arXiv:2212.10403* (2022).

[114] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019.

Gpipe: Efficient training of giant neural networks using pipeline parallelism. *NeurIPS* 32 (2019).

[115] Yafeng Huang, Huanrui Yang, Zhen Dong, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Yuan Du, Shanghang Zhang, and Kurt Keutzer. 2023. Output Sensitivity-Aware DETR Quantization. (2023).

[116] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2017. Quantized neural networks: Training neural networks with low precision weights and activations. *The Journal of Machine Learning Research* 18, 1 (2017), 6869–6898.

[117] Ajay Jaiswal, Shiwei Liu, Tianlong Chen, and Zhangyang Wang. 2023. The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter. *arXiv preprint arXiv:2306.03805* (2023).

[118] Frederick Jelinek. 1998. *Statistical methods for speech recognition*. MIT press. [119] Bo Ji and Tianyi Chen. 2022. FSCNN: A Fast Sparse Convolution Neural Network Inference System. *arXiv preprint arXiv:2212.08815* (2022). [120] Zhihao Jia, Matei Zaharia, and Alex Aiken. 2019. Beyond Data and Model Parallelism for Deep Neural Networks. Proceedings of Machine Learning and Systems 1 (2019), 1–13.

[121] Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean, Gregory R Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C Lipton, et al. 2019. Accelerating deep learning by focusing on the biggest losers. *arXiv preprint arXiv:1910.00762* (2019).

[122] Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang. 2023. Lion: Adversarial Distillation of Closed-Source Large Language Model. arXiv preprint arXiv:2305.12870 (2023).

[123] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding. *arXiv preprint arXiv:1909.10351* (2019).

[124] Sora Kadotani, Tomoyuki Kajiwara, Yuki Arase, and Makoto Onizuka. 2021. Edit distance based curriculum learning for paraphrase generation. In ACL-IJCNLP Workshop. 229–234.

[125] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. 2019. A study of BFLOAT16 for deep learning training. *arXiv preprint arXiv:1905.12322* (2019).

[126] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

2020. Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361* (2020).

[127] Angelos Katharopoulos and François Fleuret. 2017. Biased importance sampling for deep neural network training. *arXiv preprint arXiv:1706.00043* (2017). [128] Angelos Katharopoulos and François Fleuret. 2018. Not all samples are created equal: Deep learning with importance sampling. In *International conference* on machine learning. PMLR, 2525–2534.

[129] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. 2023. The Impact of Positional Encoding on Length Generalization in Transformers. *NeurIPS* (2023).

[130] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

In *NAACL-HLT*. 4171–4186.

[131] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2021. I-bert: Integer-only bert quantization. In *ICML*. PMLR, 5506–5518.

[132] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023. SqueezeLLM: Dense-andSparse Quantization. *arXiv preprint arXiv:2306.07629* (2023).

[133] Taebum Kim, Hyoungjoo Kim, Gyeong-In Yu, and Byung-Gon Chun. 2023. BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models. In *Proceedings of the 40th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 202)*, Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). 16639–16653.

[134] Yoon Kim and Alexander M Rush. 2016. Sequence-level knowledge distillation. *arXiv preprint arXiv:1606.07947* (2016). [135] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980* (2014).

[136] Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. 2019. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. *NeurIPS* 32
(2019).

[137] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2021. Reformer: The efficient transformer. *ICLR* (2021).

[138] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. 2024. DistiLLM: Towards Streamlined Distillation for Large Language Models. *arXiv preprint* arXiv:2402.03898 (2024).

[139] Tom Kocmi and Ondrej Bojar. 2017. Curriculum learning and minibatch bucketing in neural machine translation. *arXiv preprint arXiv:1707.09533* (2017).

[140] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. *NeurIPS* 35
(2022), 22199–22213.

[141] Aran Komatsuzaki. 2019. One epoch is all you need. *arXiv preprint arXiv:1906.06669* (2019). [142] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Reducing activation recomputation in large transformer models. *Proceedings of Machine Learning and Systems* 5 (2023).

[143] Atli Kosson, Vitaliy Chiley, Abhinav Venigalla, Joel Hestness, and Urs Koster. 2021. Pipelined backpropagation at scale: training large models without batches. *Proceedings of Machine Learning and Systems* 3 (2021), 479–501.

[144] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the Dark Secrets of BERT. In *EMNLP-IJCNLP*. 4365–4374.

[145] M Kumar, Benjamin Packer, and Daphne Koller. 2010. Self-paced learning for latent variable models. *NeurIPS* 23 (2010). [146] Eldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, and Dan Alistarh. 2023. Sparse Finetuning for Inference Acceleration of Large Language Models. *arXiv preprint arXiv:2310.06927* (2023).

[147] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In *Proceedings of the 29th Symposium on Operating Systems Principles*. 611–626.

[148] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. *arXiv preprint arXiv:1909.11942* (2019).

[149] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In *ICLR*.

[150] Jaejun Lee, Raphael Tang, and Jimmy Lin. 2019. What would elsa do? freezing layers during transformer fine-tuning. *arXiv preprint arXiv:1911.03090*
(2019).

[151] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. *Bioinformatics* 36, 4 (2020), 1234–1240.

[152] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. *arXiv preprint arXiv:2107.06499* (2021).

[153] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020.

Gshard: Scaling giant models with conditional computation and automatic sharding. *arXiv preprint arXiv:2006.16668* (2020).

[154] J Leskovec, A Rajaraman, and JD Ullman. 2014. Mining of Massive Datasets, Cambridge University Press, Cambridge.

[155] David D Lewis. 1995. A sequential algorithm for training text classifiers: Corrigendum and additional data. In *Acm Sigir Forum*, Vol. 29. ACM New York, NY, USA, 13–19.

[156] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:
Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In ACL. 7871–7880.

[157] Conglong Li, Minjia Zhang, and Yuxiong He. 2021. Curriculum learning: A regularization method for efficient and stable billion-scale gpt model pre-training.

(2021).

[158] Conglong Li, Minjia Zhang, and Yuxiong He. 2022. The stability-efficiency dilemma: Investigating sequence length warmup for training GPT models.

NeurIPS 35 (2022), 26736–26750.

[159] Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 2023. Symbolic Chain-of-Thought Distillation: Small Models Can Also" Think" Step-by-Step. *arXiv preprint arXiv:2306.14050* (2023).

[160] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. Scaling distributed machine learning with the parameter server. In *11th USENIX Symposium on operating systems design and implementation (OSDI 14)*. 583–598.

[161] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. 2021. Sequence parallelism: Long sequence training from system perspective.

arXiv preprint arXiv:2105.13120 (2021).

[162] Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. 2023. Functional Interpolation for Relative Positions Improves Long Context Transformers. *arXiv preprint arXiv:2310.04418* (2023).

[163] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020.

PyTorch distributed: experiences on accelerating data parallel training. *Proceedings of the VLDB Endowment* 13, 12 (2020), 3005–3018.

[164] Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. 2023. Q-diffusion: Quantizing diffusion models. *arXiv preprint arXiv:2302.04304* (2023).

[165] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. *arXiv preprint arXiv:2101.00190* (2021).

[166] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5 technical report. *arXiv preprint arXiv:2309.05463* (2023).

[167] ]literapipe Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and Ion Stoica. [n. d.]. TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models. ([n. d.]).

[168] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. *arXiv preprint arXiv:2306.00978* (2023).

[169] Hao Liu and Pieter Abbeel. 2023. Blockwise Parallel Transformer for Long Context Large Models. *arXiv preprint arXiv:2305.19370* (2023).

[170] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. *arXiv preprint arXiv:2304.08485* (2023). [171] Hao Liu, Matei Zaharia, and Pieter Abbeel. 2023. Ring Attention with Blockwise Transformers for Near-Infinite Context. *arXiv preprint arXiv:2310.01889*
(2023).

[172] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. *Comput. Surveys* 55, 9 (2023), 1–35.

[173] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, and Qi Ju. 2020. Fastbert: a self-distilling bert with adaptive inference time. arXiv preprint arXiv:2004.02178 (2020).

[174] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023. GPT understands, too. *AI Open* (2023).

[175] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692* (2019).

[176] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.

2023. LLM-QAT: Data-Free Quantization Aware Training for Large Language Models. *arXiv preprint arXiv:2305.17888* (2023).

[177] Zhining Liu, Pengfei Wei, Jing Jiang, Wei Cao, Jiang Bian, and Yi Chang. 2020. MESA: boost ensemble imbalanced learning with meta-sampler. *NeurIPS* 33
(2020), 14463–14474.

[178] Jieyi Long. 2023. Large Language Model Guided Tree-of-Thought. *arXiv preprint arXiv:2305.08291* (2023).

[179] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. *arXiv preprint arXiv:1711.05101* (2017).

[180] Xiuqing Lv, Peng Zhang, Sunzhu Li, Guobing Gan, and Yueheng Sun. 2023. LightFormer: Light-weight Transformer Using SVD-based Weight Transfer and Parameter Sharing. In *Findings of the Association for Computational Linguistics: ACL 2023*. 10323–10335.

[181] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. LLM-Pruner: On the Structural Pruning of Large Language Models. *arXiv preprint arXiv:2305.11627*
(2023).

[182] Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and Dawei Song. 2019. A tensorized transformer for language modeling.

Advances in neural information processing systems 32 (2019).

[183] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. 2023. Mega: moving average equipped gated attention. *ICLR* (2023).

[184] Seiji Maekawa, Dan Zhang, Hannah Kim, Sajjadur Rahman, and Estevam Hruschka. 2022. Low-resource interactive active labeling for fine-tuning language models. In *Findings of EMNLP*. 3230–3242.

[185] Katerina Margatina, Giorgos Vernikos, Loïc Barrault, and Nikolaos Aletras. 2021. Active learning by acquiring contrastive examples. *arXiv preprint* arXiv:2109.03764 (2021).

[186] Pedro Henrique Martins, Zita Marinho, and André FT Martins. 2022.    -former: Infinite Memory Transformer. ACL (2022).

[187] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. 2023. Long range language modeling via gated state spaces. *ICLR* (2023).

[188] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. 2017. Mixed precision training. *arXiv preprint arXiv:1710.03740* (2017).

[189] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model.. In `
Interspeech, Vol. 2. Makuhari, 1045–1048.

[190] Tomáš Mikolov, Stefan Kombrink, Lukáš Burget, Jan Černocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In `
2011 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 5528–5531.

[191] Swaroop Mishra and Bhavdeep Singh Sachdeva. 2020. Do we need to create big datasets to learn a task?. In *SustaiNLP Workshop*. 169–173. [192] Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.

2023. Scaling Data-Constrained Language Models. *arXiv preprint arXiv:2305.16264* (2023).

[193] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. 2022. Multimodal contrastive learning with limoe: the language-image mixture of experts. *NeurIPS* 35 (2022), 9564–9576.

[194] Lakshmi Nair, Mikhail Bernadskiy, Arulselvan Madhavan, Craig Chan, Ayon Basumallik, and Darius Bunandar. 2023. INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers. *arXiv preprint arXiv:2307.03712* (2023).

[195] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. 2019.

PipeDream: Generalized pipeline parallelism for DNN training. In *Proceedings of the 27th ACM Symposium on Operating Systems Principles*. 1–15.

[196] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. 2021. Memory-efficient pipeline-parallel dnn training. In International Conference on Machine Learning. PMLR, 7937–7947.

[197] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 1–15.

[198] Matan Ben Noach and Yoav Goldberg. 2020. Compressing pre-trained language models by matrix decomposition. In *Proceedings of the 1st Conference of the* Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing. 884–889.

[199] Sangyun Oh, Hyeonuk Sim, Jounghyun Kim, and Jongeun Lee. 2022. Non-Uniform Step Size Quantization for Accurate Post-Training Quantization. In European Conference on Computer Vision. Springer, 658–673.

[200] OpenAI. 2023. GPT-4 Technical Report. *ArXiv* abs/2303.08774 (2023).

[201] OpenAI. 2023. Introducing ChatGPT. (2023). https://openai.com/blog/chatgpt
[202] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. *NeurIPS* 35 (2022), 27730–27744.

[203] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP Models really able to Solve Simple Math Word Problems?. In ACL. ACL, 2080–2094.

[204] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al.

2023. RWKV: Reinventing RNNs for the Transformer Era. *arXiv preprint arXiv:2305.13048* (2023).

[205] Bo Peng, Ben Burns, Ziqi Chen, Srinivasan Parthasarathy, and Xia Ning. 2023. Towards Efficient and Effective Adaptation of Large Language Models for Sequential Recommendation. *arXiv preprint arXiv:2310.01612* (2023).

[206] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. *arXiv preprint* arXiv:2309.00071 (2023).

[207] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In *NAACL-HLT*. 2227–2237.

[208] Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020. Adapterhub: A
framework for adapting transformers. *arXiv preprint arXiv:2007.07779* (2020).

[209] Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. Mad-x: An adapter-based framework for multi-task cross-lingual transfer. *arXiv* preprint arXiv:2005.00052 (2020).

[210] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. 2023. Hyena hierarchy: Towards larger convolutional language models. *arXiv preprint arXiv:2302.10866* (2023).

[211] Ofir Press, Noah A Smith, and Mike Lewis. 2020. Shortformer: Better language modeling using shorter inputs. *arXiv preprint arXiv:2012.15832* (2020).

[212] Ofir Press, Noah A Smith, and Mike Lewis. 2023. Train short, test long: Attention with linear biases enables input length extrapolation. *ICLR* (2023). [213] Joseph Prusa, Taghi M Khoshgoftaar, David J Dittman, and Amri Napolitano. 2015. Using random undersampling to alleviate class imbalance on tweet sentiment data. In *2015 IEEE international conference on information reuse and integration*. IEEE, 197–202.

[214] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).

[215] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners.

OpenAI blog 1, 8 (2019), 9.

[216] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. *arXiv preprint arXiv:2112.11446* (2021).

[217] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. *The Journal of Machine Learning Research* 21, 1 (2020), 5485–5551.

[218] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *Journal of Machine Learning Research* 21, 140 (2020), 1–67.

[219] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022.

Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale. In *ICML*. 18332–18346.

[220] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC.

1–16.

[221] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In *SIGKDD*. 3505–3506.

[222] Machel Reid, Edison Marrese-Taylor, and Yutaka Matsuo. 2021. Subformer: Exploring weight sharing for parameter efficiency in generative transformers.

arXiv preprint arXiv:2101.00234 (2021).

[223] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload:
Democratizing Billion-Scale Model Training. *arXiv preprint arXiv:2101.06840* (2021).

[224] Joshua Robinson and David Wingate. 2022. Leveraging Large Language Models for Multiple Choice Question Answering. In *ICLR*.

[225] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. 2014. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 (2014).

[226] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. 2019. A constructive prediction of the generalization error across scales. *arXiv* preprint arXiv:1909.12673 (2019).

[227] Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here? *IEEE* 88, 8 (2000), 1270–1278.

[228] Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. 2021. AdapterDrop: On the Efficiency of Adapters in Transformers. In *EMNLP*. 7930–7946.

[229] Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane Legg, and Joel Veness. 2023. Randomized Positional Encodings Boost Length Generalization of Transformers. *arXiv preprint arXiv:2305.16843* (2023).

[230] Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, and Issam H Laradji. 2023. Promptmix: A class boundary augmentation method for large language model distillation. *arXiv preprint arXiv:2310.14192* (2023).

[231] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. 2013. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In *2013 IEEE international conference on acoustics, speech and signal processing*. IEEE, 6655–6659.

[232] Gerard Salton and Chris Buckley. 1990. Improving retrieval performance by relevance feedback. *Journal of the American society for information science* 41, 4 (1990), 288–297.

[233] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).

[234] AMissing SECRET SAUCE. 2023. OUTLIER WEIGHED LAYERWISE SPARSITY (OWL): AMissing SECRET SAUCE FOR PRUNING LLMS TO HIGH
SPARSITY. (2023).

The Efficiency Spectrum of Large Language Models: An Algorithmic Survey Efficient LLM Algorithmic Survey, Nov, 2023, USA.

[235] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. *arXiv preprint arXiv:2211.05100* (2022).

[236] Ozan Sener and Silvio Savarese. 2017. Active learning for convolutional neural networks: A core-set approach. *arXiv preprint arXiv:1708.00489* (2017). [237] Burr Settles. 2009. Active learning literature survey. University of Wisconsin-Madison Department of Computer Sciences.

[238] Burr Settles. 2011. From theories to queries: Active learning in practice. In *Active learning and experimental design workshop in conjunction with AISTATS*
2010. JMLR Workshop and Conference Proceedings, 1–18.

[239] Burr Settles. 2012. Active Learning. In *Synthesis Lectures on Artificial Intelligence and Machine Learning*. 1–114. [240] Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. 1992. Statistical mechanics of learning from examples. *Physical review A* 45, 8 (1992),
6056.

[241] Murray Shanahan. 2022. Talking about large language models. *arXiv preprint arXiv:2212.03551* (2022).

[242] Hang Shao, Bei Liu, and Yanmin Qian. 2023. One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. *arXiv preprint arXiv:2310.09499*
(2023).

[243] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. 2018. Mesh-tensorflow: Deep learning for supercomputers. *Advances in neural information processing systems* 31 (2018).

[244] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. *ICLR* (2017).

[245] Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In *International Conference on Machine Learning*.

PMLR, 4596–4604.

[246] Li Shen, Yan Sun, Zhiyuan Yu, Liang Ding, Xinmei Tian, and Dacheng Tao. 2023. On Efficient Training of Large-Scale Deep Learning Models: A Literature Review. *arXiv preprint arXiv:2304.03589* (2023).

[247] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based ultra low precision quantization of bert. In *AAAI*, Vol. 34. 8815–8821.

[248] Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, et al. 2023.

Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts. *arXiv preprint arXiv:2305.14705* (2023).

[249] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. *arXiv preprint arXiv:2010.15980* (2020).

[250] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. *arXiv preprint arXiv:1909.08053* (2019).

[251] Aditya Siddhant and Zachary C Lipton. 2018. Deep bayesian active learning for natural language processing: Results of a large-scale empirical study. *arXiv* preprint arXiv:1808.05697 (2018).

[252] Shoaib Ahmed Siddiqui, Nitarshan Rajkumar, Tegan Maharaj, David Krueger, and Sara Hooker. 2022. Metadata archaeology: Unearthing data subsets by leveraging training dynamics. *arXiv preprint arXiv:2209.10015* (2022).

[253] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. 2023. Simplified state space layers for sequence modeling. *ICLR*.

[254] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. 2022. Beyond neural scaling laws: beating power law scaling via data pruning. *NeurIPS* 35 (2022), 19523–19536.

[255] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022.

Selective annotation makes language models better few-shot learners. *arXiv preprint arXiv:2209.01975* (2022).

[256] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding.

arXiv preprint arXiv:2104.09864 (2021).

[257] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355 (2023).

[258] Shivakanth Sujit, Somjit Nath, Pedro HM Braga, and Samira Ebrahimi Kahou. 2022. Prioritizing samples in reinforcement learning with reducible loss.

arXiv preprint arXiv:2208.10483 (2022).

[259] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023. A Simple and Effective Pruning Approach for Large Language Models. arXiv preprint arXiv:2306.11695 (2023).

[260] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023. Retentive network: A successor to transformer for large language models. *arXiv preprint arXiv:2307.08621* (2023).

[261] Zhiqing Sun, Yiming Yang, and Shinjae Yoo. 2022. Sparse attention with learning to hash. In *ICLR*.

[262] Marzieh S Tahaei, Ella Charlaix, Vahid Partovi Nia, Ali Ghodsi, and Mehdi Rezagholizadeh. 2021. Kroneckerbert: Learning kronecker decomposition for pre-trained language models via knowledge distillation. *arXiv preprint arXiv:2109.06243* (2021).

[263] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In ACL. ACL, Minneapolis, Minnesota, 4149–4158.

[264] Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active learning for statistical natural language parsing. In ACL. 120–127.

[265] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca:
An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca.

[266] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. 2022. Scaling laws vs model architectures: How does inductive bias influence scaling? *arXiv preprint arXiv:2207.10551* (2022).

[267] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. Efficient Transformers: A Survey. *ACM Comput. Surv.* 55, 6, Article 109 (dec 2022),
28 pages.

[268] Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. 2021. Scale Efficiently: Insights from Pretraining and Finetuning Transformers. In *International Conference on Learning Representations*.

[269] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.

2022. Lamda: Language models for dialog applications. *arXiv preprint arXiv:2201.08239* (2022).

[270] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971* (2023).

[271] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288* (2023).

[272] Marcos Treviso, António Góis, Patrick Fernandes, Erick Fonseca, and André FT Martins. 2021. Predicting attention sparsity in transformers. arXiv preprint arXiv:2109.12188 (2021).

[273] Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, et al. 2023. Efficient methods for natural language processing: A survey. *TACL* 11 (2023), 826–860.

[274] Aimee Van Wynsberghe. 2021. Sustainable AI: AI for sustainability and the sustainability of AI. *AI and Ethics* 1, 3 (2021), 213–218. [275] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. *NeurIPS* 30 (2017).

[276] Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans, Max Tegmark, and Francesco Fuso Nerini. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals. *Nature communications* 11, 1
(2020), 1–10.

[277] Yu Wan, Baosong Yang, Derek F Wong, Yikai Zhou, Lidia S Chao, Haibo Zhang, and Boxing Chen. 2020. Self-paced learning for neural machine translation.

arXiv preprint arXiv:2010.04505 (2020).

[278] Boxiang Wang, Qifan Xu, Zhengda Bian, and Yang You. 2022. Tesseract: Parallelize the tensor parallelism efficiently. In *ICPP*. 1–11.

[279] Jing Wang, Jie Shen, Xiaofei Ma, and Andrew Arnold. 2022. Uncertainty-based active learning for reading comprehension. *TMLR* (2022).

[280] Jianing Wang, Chengyu Wang, Fuli Luo, Chuanqi Tan, Minghui Qiu, Fei Yang, Qiuhui Shi, Songfang Huang, and Ming Gao. 2022. Towards unified prompt tuning for few-shot text classification. *arXiv preprint arXiv:2205.05313* (2022).

[281] Minjie Wang, Chien-chin Huang, and Jinyang Li. 2018. Supporting Very Large Models using Automatic Dataflow Graph Partitioning. (2018).

[282] Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-consistent chain-of-thought distillation. arXiv preprint arXiv:2305.01879 (2023).

[283] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In *ICLR*.

[284] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. *arXiv preprint arXiv:2212.10560* (2022).

[285] Yile Wang, Yue Zhang, Peng Li, and Yang Liu. 2022. Language Model Pre-training with Linguistically Motivated Curriculum Learning. (2022).

[286] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.

Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent Abilities of Large Language Models. *TMLR* (2022).

Survey Certification.

[287] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. *NeurIPS* 35 (2022), 24824–24837.

[288] Ulme Wennberg and Gustav Eje Henter. 2021. The case for translation-invariant self-attention in transformer-based language models. arXiv preprint arXiv:2106.01950 (2021).

[289] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Édouard Grave. 2020. CCNet:
Extracting High Quality Monolingual Datasets from Web Crawl Data. In *Proceedings of The 12th Language Resources and Evaluation Conference*. 4003–4012.

[290] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A
prompt pattern catalog to enhance prompt engineering with chatgpt. *arXiv preprint arXiv:2302.11382* (2023).

[291] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al.

2022. Sustainable ai: Environmental implications, challenges and opportunities. *Proceedings of Machine Learning and Systems* 4 (2022), 795–813.

[292] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2023. Lamini-lm: A diverse herd of distilled models from large-scale instructions. *arXiv preprint arXiv:2304.14402* (2023).

The Efficiency Spectrum of Large Language Models: An Algorithmic Survey Efficient LLM Algorithmic Survey, Nov, 2023, USA.

[293] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.

2023. Bloomberggpt: A large language model for finance. *arXiv preprint arXiv:2303.17564* (2023).

[294] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. 2023. Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity. *arXiv preprint arXiv:2309.10285* (2023).

[295] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023. Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning.

arXiv preprint arXiv:2310.06694 (2023).

[296] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In *ICML*. PMLR, 38087–38099.

[297] Shuo Xie, Jiahao Qiu, Ankita Pasad, Li Du, Qing Qu, and Hongyuan Mei. 2022. Hidden state variability of pretrained language models can guide computation reduction for transfer learning. In *EMNLP*.

[298] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. 2023. Data selection for language models via importance resampling. *arXiv preprint* arXiv:2302.03169 (2023).

[299] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 2023. Decomposition enhances reasoning via self-evaluation guided decoding. *arXiv preprint arXiv:2305.00633* (2023).

[300] Eric P Xing, Qirong Ho, Wei Dai, Jin-Kyu Kim, Jinliang Wei, Seunghak Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, and Yaoliang Yu. 2015. Petuum: A
new platform for distributed machine learning on big data. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 1335–1344.

[301] Canwen Xu and Julian McAuley. 2023. A survey on model compression and acceleration for pretrained language models. In *AAAI*, Vol. 37. 10566–10575.

[302] Qifan Xu and Yang You. 2023. An efficient 2d method for training super-large deep learning models. In 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEE, 222–232.

[303] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. 2021. GSPMD: general and scalable parallelization for ML computation graphs. *arXiv preprint arXiv:2105.04663* (2021).

[304] Ziyun Xu, Chengyu Wang, Minghui Qiu, Fuli Luo, Runxin Xu, Songfang Huang, and Jun Huang. 2023. Making pre-trained language models end-to-end few-shot learners with contrastive prompt tuning. In *Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining*. 438–446.

[305] Zhao Xu, Kai Yu, Volker Tresp, Xiaowei Xu, and Jizhi Wang. 2003. Representative sampling for text classification using support vector machines. In *ECIR*.

393–407.

[306] Bowen Yang, Jian Zhang, Jonathan Li, Christopher Ré, Christopher Aberger, and Christopher De Sa. 2021. Pipemare: Asynchronous pipeline parallel dnn training. *Proceedings of Machine Learning and Systems* 3 (2021), 269–296.

[307] Haichuan Yang, Shupeng Gui, Yuhao Zhu, and Ji Liu. 2020. Automatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2178–2188.

[308] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: A survey on chatgpt and beyond. *arXiv preprint arXiv:2304.13712* (2023).

[309] Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei Cheng. 2023. Exploring the limits of chatgpt for query or aspect-based text summarization.

arXiv preprint arXiv:2302.08081 (2023).

[310] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. *arXiv preprint arXiv:2305.10601* (2023).

[311] Xingcheng Yao, Yanan Zheng, Xiaocong Yang, and Zhilin Yang. 2022. Nlp from scratch without large-scale pretraining: A simple and efficient framework.

In *International Conference on Machine Learning*. PMLR, 25438–25451.

[312] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. *NeurIPS* 35 (2022), 27168–27183.

[313] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. 2023. EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models. *arXiv preprint arXiv:2308.14352* (2023).

[314] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. 2017. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In *CVPR*. 4133–4141.

[315] Yue Yu, Lingkai Kong, Jieyu Zhang, Rongzhi Zhang, and Chao Zhang. 2022. AcTune: Uncertainty-based active self-training for active fine-tuning of pretrained language models. In *NAACL*. 1422–1436.

[316] Michelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-Graber. 2020. Cold-start active learning through self-supervised language modeling. arXiv preprint arXiv:2010.09535 (2020).

[317] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. 2023. RPTQ:
Reorder-based Post-training Quantization for Large Language Models. *arXiv preprint arXiv:2304.01089* (2023).

[318] Sergey Zagoruyko and Nikos Komodakis. 2016. Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer. In *ICLR*.

[319] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. *NeurIPS* (2020), 17283–17297.

[320] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models.

In ACL. 1–9.

[321] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326 (2018).

[322] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. 2023. Data-centric artificial intelligence: A
survey. *arXiv preprint arXiv:2303.10158* (2023).

[323] ChengXiang Zhai et al. 2008. Statistical language models for information retrieval a critical review. Foundations and Trends® *in Information Retrieval* 2, 3
(2008), 137–213.

[324] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. 2021. An attention free transformer.

arXiv preprint arXiv:2105.14103 (2021).

[325] Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, and Dawei Song. 2023. Lifting the Curse of Capacity Gap in Distilling Language Models. *arXiv preprint arXiv:2305.12129* (2023).

[326] Jiong Zhang, Hsiang-Fu Yu, and Inderjit S Dhillon. 2019. Autoassist: A framework to accelerate training of deep neural networks. Advances in Neural Information Processing Systems 32 (2019).

[327] Mingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al. 2023. Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning.

arXiv preprint arXiv:2305.18403 (2023).

[328] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. 2023. Adaptive budget allocation for parameter-efficient fine-tuning. *arXiv preprint arXiv:2303.10512* (2023).

[329] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction Tuning for Large Language Models: A Survey. *arXiv preprint arXiv:2308.10792* (2023).

[330] Shujian Zhang, Chengyue Gong, Xingchao Liu, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. 2022. Allsh: Active learning guided by local sensitivity and hardness. *arXiv preprint arXiv:2205.04980* (2022).

[331] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.

2022. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068* (2022).

[332] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812 (2020).

[333] Yuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, and Rongrong Ji. 2023. Dynamic Sparse No Training:
Training-Free Fine-tuning for Sparse LLMs. *arXiv preprint arXiv:2310.08915* (2023).

[334] Mingjun Zhao, Haijiang Wu, Di Niu, and Xiaoli Wang. 2020. Reinforced curriculum learning on pre-trained neural machine translation models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 9652–9659.

[335] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
survey of large language models. *arXiv preprint arXiv:2303.18223* (2023).

[336] Fedor Zhdanov. 2019. Diverse mini-batch active learning. *arXiv preprint arXiv:1901.05954* (2019).

[337] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al.

2022. Alpa: Automating inter-and {Intra-Operator} parallelism for distributed deep learning. In *16th USENIX Symposium on Operating Systems Design and* Implementation (OSDI 22). 559–578.

[338] Zhehua Zhong, Tianyi Chen, and Zhen Wang. 2023. MAT: mixed-strategy game of adversarial training in fine-tuning. *arXiv preprint arXiv:2306.15826*
(2023).

[339] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li. 2021. Learning n: m fine-grained structured sparse neural networks from scratch. *arXiv preprint arXiv:2102.04010* (2021).

[340] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. 2023. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. *arXiv preprint arXiv:2302.09419* (2023).

[341] Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu. 2022. On the Optimization Landscape of Neural Collapse under MSE Loss:
Global Optimality with Unconstrained Features. In *Proceedings of the 39th International Conference on Machine Learning*. 27179–27202.

[342] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. 2016. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. *arXiv preprint arXiv:1606.06160* (2016).

[343] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023. Large Language Models are Human-Level Prompt Engineers. In *ICLR*.

[344] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. *arXiv preprint arXiv:2304.10592* (2023).

[345] Qingqing Zhu, Xiuying Chen, Pengfei Wu, JunFei Liu, and Dongyan Zhao. 2021. Combining curriculum learning and knowledge distillation for dialogue generation. In *Findings of the Association for Computational Linguistics: EMNLP 2021*. 1284–1295.

[346] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. A Survey on Model Compression for Large Language Models. *arXiv preprint arXiv:2308.07633*
(2023).

The Efficiency Spectrum of Large Language Models: An Algorithmic Survey Efficient LLM Algorithmic Survey, Nov, 2023, USA.

[347] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. 2021. A geometric analysis of neural collapse with unconstrained features. *Advances in Neural Information Processing Systems* (2021), 29820–29834.

[348] Zeyuan Allen Zhu and Yuanzhi Li. 2023. Physics of Language Models: Part 3.1, Knowledge Storage and Extraction. arXiv:2309.14316 [cs.CL]
[349] Bohan Zhuang, Jing Liu, Zizheng Pan, Haoyu He, Yuetian Weng, and Chunhua Shen. 2023. A survey on efficient training of transformers. *arXiv preprint* arXiv:2302.01107 (2023).

[350] Yimeng Zhuang, Jing Zhang, and Mei Tu. 2022. Long-range Sequence Modeling with Predictable Sparse Attention. In ACL. 234–243.

[351] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. *arXiv preprint arXiv:1909.08593* (2019).

[352] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. 2022. Designing effective sparse expert models. *IPDPSW* (2022).