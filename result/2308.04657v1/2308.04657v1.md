# Which Tokens To Use? Investigating Token Reduction In Vision Transformers

Joakim Bruslund Haurum1 Sergio Escalera2,1 Graham W. Taylor3 Thomas B. Moeslund1 1 Visual Analysis and Perception (VAP) Laboratory, Aalborg University & Pioneer Centre for AI, Denmark 2 Universitat de Barcelona & Computer Vision Center, Spain 3 University of Guelph & Vector Institute for AI, Canada joha@create.aau.dk, sescalera@ub.edu, gwtaylor@uoguelph.ca, tbm@create.aau.dk

## Abstract

Since the introduction of the Vision Transformer (ViT),
researchers have sought to make ViTs more efficient by removing redundant information in the processed tokens.

While different methods have been explored to achieve this goal, we still lack understanding of the resulting reduction patterns and how those patterns differ across token reduction methods and datasets. To close this gap, we set out to understand the reduction patterns of 10 different token reduction methods using four image classification datasets.

By systematically comparing these methods on the different classification tasks, we find that the Top-K pruning method is a surprisingly strong baseline. Through in-depth analysis of the different methods, we determine that: the reduction patterns are generally not consistent when varying the capacity of the backbone model, the reduction patterns of pruning-based methods significantly differ from fixed radial patterns, and the reduction patterns of pruning-based methods are correlated across classification datasets. Finally we report that the similarity of reduction patterns is a moderate-to-strong proxy for model performance. Project page at *https://vap.aau.dk/tokens*.

## 1. Introduction

The Vision Transformer (ViT) [15] has in record time seen wide spread adoption within computer vision, ousting Convolutional Neural Networks (CNNs). In order to better understand how ViTs function, prior works have investigated whether ViTs process data in a similar way as CNNs [49], and how different types of supervision affect ViT training [62]. In this work we investigate the use of token reduction methods, which leverage the fact that ViTs can accommodate variable input sequence lengths. These methods aim to make ViTs more efficient by removing redundant tokens and thereby reduce the computational cost of the self-attention operation, which scales quadratically with the number of tokens [5, 18, 50].

However, with some limited exceptions, little has been done to gain deeper insights into how the token reduction

![0_image_0.png](0_image_0.png)

Figure 1: **Average method rank.** The average rank of each tested method plotted with ± 1 standard deviation. The Top-K pruning method and its extension, EViT, are found to be the best performing methods.

process differs across methods or depends on hyperparameters such as backbone capacity or the number of tokens to be kept. Furthermore, the analysis of methods have primarily been constricted to the ImageNet dataset, and is rarely done with structured comparisons to other methods. Consequently, the community does not have a good understanding of how the methods differ from one another. We set out to rectify this by conducting a systematic comparison of 10 recently proposed methods, leveraging thorough experiments to elucidate the inner workings of reduction processes. In this work we make the following contributions:
- We conduct the first systematic comparison and analysis of 10 state-of-the-art token reduction methods across four image classification datasets, trained using a single codebase and consistent training protocol.

- We find that the Top-K and EViT methods are strong baselines across all datasets.

- Extensive experiments providing deeper insights into the core mechanisms of the token reduction methods.

- We find that the similarity in reduction patterns is a moderate-to-strong proxy for model performance.

## 2. Related Works

Efficient Vision Transformers. Since the introduction of the Transformer [61] and Vision Transformer [15] a large number of methods have been proposed to make the Transformer model more efficient. Within the computer vision domain several method paradigms have been investigated such as model pruning [7, 9, 46, 56, 71] and quantization [35, 39], structured token downsampling inspired by the pooling layers in CNNs [20, 23, 41], randomly dropping patches [1, 40], part selection modules
[22, 26, 27, 63, 66], and dynamic resizing of input patches [4, 8, 37, 64, 65, 72, 77]. Additionally, the Transformer model uniquely allows for variable input sequence lengths, enabling a new type of method, called token reduction, which operate directly on the token sequence. These methods are the focus of this work.

Token Reduction Methods. The prior work on sparsification of the input token sequence can be divided into two primary paradigms: token pruning [16, 18, 30, 34, 36, 42, 48, 50, 58, 69, 70] and token merging [5, 21, 45, 51, 55, 67, 68, 74, 78]. Pruning-based methods aim to reduce the token sequence by removing tokens, whereas merging-based methods reduce the token sequence by combining tokens.

Pruning-based methods can be split into dynamic and static keep rate methods, depending on whether the method can dynamically choose how many tokens to prune. Static keep rate pruning methods select a predetermined number of tokens to keep by using the attention scores between the spatial tokens and the global class (CLS) token [36, 42] or by predicting per-token keep probabilities
[30, 50]. In order to not completely discard the information in the pruned tokens, which can contain contextual information regarding e.g. location and background, several methods propose merging the pruned tokens into a single token
[30, 36, 42, 69]. On the other hand, dynamic keep rate pruning methods select an adaptive number of tokens to keep by using either sampling methods [18, 70], reinforcement learning [48], or alternating training schemes [34, 58].

Similarly, merging-based methods can be split into hardmerging [5, 45, 68, 74] and soft-merging [21, 51, 67, 78],
depending on whether the merging operation requires the token assignment to be mutually exclusive. Hard-merging methods typically use commonly known clustering methods such as K-Means [45], K-Medoids [45], and Density-Peak Clustering with K-Nearest Neighbours (DPC-KNN) [74].

Other hard-merging based methods have used bipartitematching of tokens [5] and cross-attention between spatial tokens and learnable cluster centers, called queries [68].

The soft-merging based approaches have primarily consisted of soft-clustering methods, which lead to a convex combination of spatial tokens derived from similarity with queries or the spatial tokens themselves [21, 51, 78].

To summarize, while many different token reduction methods have been proposed, scant attention has been given to comparing the methods in a systematic way, nor trying to better understand how the reduction process and reduction patterns (i.e. constructed clusters and pruned tokens)
are affected by the choice of reduction method, datasets, and model settings. In this work we aim to rectify this by conducting a thorough systematic comparison and in-depth analysis of contemporary token reduction methods.

## 3. Experimental Design

In order to compare the different token reduction methods fairly, we select representative methods from each reduction method paradigm; see Section 3.1. The methods are chosen based on two criteria: 1) the selected methods should cover the key trends within each paradigm, and 2)
the methods should be inserted into the backbone with minimal adjustments to the training loop.

Several ways of incorporating the token reduction operation have previously been used, ranging from a single reduction stage in the middle of the ViT [34, 51] to after each stage in the ViT [5, 18, 45, 70]. However, the most common approach is to apply the token reduction operation at three predefined stages dividing the ViT into four sections of equal size [30, 36, 42, 48, 50, 68, 74, 78]. This is the setting which we will follow. At each stage a ratio of the tokens, r, are kept for further processing, where r ∈ {0.25, 0.50, 0.70, 0.90} is denoted the *keep rate*.

## 3.1. Methods

To ensure diversity of methods we select three representative and top-performing methods from each paradigm.

For the Dynamic Keep Rate Pruning paradigm, however, we only select one as the other methods either did not converge during training with the training settings described in Section 3.3 (A-ViT [70]), or required substantial modifications to the training loop (IA-RED2[48], DPS-ViT [58], and SaiT
[34]). Each method is described in Section 3.1.2–3.1.5. We also introduce a set of baseline pruning methods with a fixed image-centered radial pattern, see Section 3.1.1. These are based on observations made by Yin *et al*. [70] and Rao et al. [50] who found that the averaged reduction patterns display a radial pattern focused on the image center. All methods are implemented in a single codebase based on official model implementations when possible.

## 3.1.1 Fixed Pattern Pruning Baseline

We introduce a set of baseline methods with a fixed reduction pattern based on the distance of each token to the center of the image, measured using the ℓp-norm. Specifically, we consider fixed patterns created by using the ℓ1, ℓ2, and ℓ∞
norms. In order to create the fixed patterns such that only r s tokens are kept at reduction stages s ∈ {1, 2, 3}, we prune tokens based on their distance to the image center, setting the threshold such that the absolute difference between the kept tokens and P rsis minimized, where P is the initial amount of spatial tokens.

## 3.1.2 Static Keep Rate Pruning

Top-K is a commonly used pruning baseline, where the attention between the P spatial tokens and the CLS token is used. At reduction stage s the method simply selects the Ks most attended to tokens, where Ks = P rs.

EViT [36] extends Top-K pruning by creating a single
"fused" token at each stage s. The fused token is computed by averaging the Ps − Ks pruned tokens weighted by their CLS token attention scores, where Ps is the number of tokens at stage s before the reduction is applied.

DynamicViT [50] prunes the tokens by constructing a binary decision mask Ds based on keep probabilities produced by a small prediction module. The Gumbel-Softmax trick [29] is used to ensure training is differentiable, while during inference the Ks most probable tokens are kept. An extra loss is needed to ensure that Ds only keeps Ks tokens.

## 3.1.3 Dynamic Keep Rate Pruning

ATS [18] is a sampling-based pruning method which selects a variable amount of tokens at each reduction stage. This is achieved by applying the inverse transform sampling (ITS)
on the cumulative distribution function (CDF) of the CLS
token attention scores and uniformly sampling the CDF P rs times. In case a token is assigned a high attention score by the CLS token it may be sampled multiple times by the ITS
operation, in which case only a single copy is kept. Thereby, ATS can sample fewer than P rstokens at stage s.

## 3.1.4 Hard-Merging

ToMe [5] is a recent token merging method, where the set of tokens are split into a bipartite graph with equal sized partitions A and B, where edges are constructed by drawing a single edge for each node in A to the node in B with the highest cosine similarity. The Ps(1−r s) highest valued edges are kept and connected nodes are merged by averaging the token features, followed by combining set A and B again. It should be noted that the ToMe method is constrained such that r cannot be below 50% as nodes in set A
are only allowed a single edge. In order to align the nomenclature with clustering methods, we denote the output of the ToMe method as cluster centers. K-Medoids [45] is an iterative hard-clustering baseline where the P rscluster centers are set to be the cluster element which minimizes the ℓ2 distance to all other elements in the cluster. The method iteratively updates the clusters by assigning tokens to the cluster with the closest cluster center. We initialize the cluster centers based on the CLS
token attention scores as proposed by Marin *et al*. [45]. DPC-KNN [17] is a two-step clustering approach, where first the density of each token is computed based on the distance to the k-nearest neighbours, followed by determining the minimum distance to a point with higher density.

The two measures are combined and the cluster centers are defined to be the P rstokens with the highest combined scores. The final cluster representation is obtained by averaging the elements assigned to the cluster. Zeng *et al*. [74]
proposed to add a small linear layer which predicts the importance for each token, making the cluster representation a weighted average of the cluster elements.

## 3.1.5 Soft-Merging

SiT [78] is a recent soft-clustering method, where a small network predicts an assignment matrix As, representing a convex combination of the P rs−1input tokens to construct P rsclusters. Specifically Xs = Xs−1As, where PP rs i=1 As[*i, j*] = 1 for j = 1, 2*, . . . , P r*s−1and X is the token feature representations.

Sinkhorn [21] is a query-based clustering method, where unlike in SiT, the cluster centers, called queries, are randomly initialized learnable vectors. The assignment matrix is constructed by applying the Sinkhorn-Knopp algorithm on the cosine similarities between the tokens and queries.

PatchMerger [51] is a query-based clustering method, similar to Sinkhorn, where the assignment matrix is constructed by calculating the dot product between the queries and tokens. This is followed by a softmax operation to ensure the assignment matrix results in a convex combination.

## 3.2. Datasets

Previously, methods have only been tested on the ImageNet [54] classification dataset and primarily against the backbone model with no token reduction methods. In order to gain diverse insights into the methods, we analyse and compare the different token reduction methods using four distinct classification datasets: ImageNet, NABirds [60],
COCO [38], and NUS-WIDE [10].

ImageNet and NABirds are used to evaluate the commonly used multi-class classification task. ImageNet is the most commonly used vision classification dataset, consisting of 1000 diverse classes across 1.2 million images. In contrast, the NABirds dataset represents a much more finegrained classification task, consisting of 48,000 images and 555 bird classes. We also compare methods on the COCO
and NUS-WIDE multi-label classification tasks, representing the case where more than one class of interest can be present simultaneously. COCO and NUS-WIDE consists of 80–81 classes of common object and animals across 122k to 220k images, respectively. In contrast to ImageNet, where the object of interest is often in the center of the image, the NABirds, COCO, and NUS-WIDE represent scenarios where the distinguishing attributes are not necessarily in the center of the image, or there may be more than one object of interest, respectively. Example images of each dataset are shown in Fig. 2. Classification performance on ImageNet and NABirds is measured using the standard Top-1 accuracy metric [22, 54], and for COCO and NUS-WIDE we report the mean Average Precision score (mAP) [52].

## 3.3. Training Details

All methods are inserted into an DeiT backbone pretained on ImageNet without distillation [59] at the 4th, 7th, and 10th transformer blocks. The DeiT backbone was chosen as it is the most commonly used throughout the token reduction literature. We consider both the Tiny, Small, and Base DeiT backbone capacities, denoted DeiT-{T, S, B},
respectively.

For all methods we based our hyperparameter settings on those presented by Rao *et al*. [50]. A hyperparameter search over the learning rate warmup period, backbone learning rate scaler, and backbone freeze period was initially conducted on the ImageNet dataset using the DeiT-S backbone, training for 30 epochs. The best performing setting for each r was used for training the DeiT-T and DeiT-B variants. It should be noted that for the DynamicViT and SiT methods we do not include the distillation losses used in the original papers, as we find that the effect is minimal and instead choose to keep the training procedure consistent.

For NABirds, COCO, and NUS-WIDE, we fine-tuned the DeiT-S baseline in a similar manner, and for each token reduction method compared the ImageNet hyperparameter setting and fine-tuned DeiT-S hyperparameters. The best set of hyperparameters was used to train the DeiT-T and DeiT-B variants. The NABirds models were trained for 50 epochs with minimal augmentation and no label smoothing, following the guidelines from He *et al*. [22]. COCO
and NUS-WIDE models were trained for 40 epochs with the Asymmetric Loss [52] following the multi-label classification guidelines from Ben-Baruch *et al*. [52]. The specific hyperparameter values can be found in the supplementary materials (supp. mat.).

Lastly, it should be noted that for all datasets we trained the models at a resolution of 224×224. This is non-standard for the NABirds, COCO, and NUS-WIDE datasets (normally 448 × 448). This choice was made to keep reduction patterns comparable, and because the aim was not to push the state-of-the-art in accuracy, but rather to train a set of models from which we can gain deeper insights.

## 4. Results

We report performance on the four image classification datasets considered with the DeiT-S backbone results in Ta-

![3_image_0.png](3_image_0.png)

ble 1, results for the DeiT-T and DeiT-B backbones in the supp. mat., and the average rank of the methods in Figure 1. Across all backbone capacities we note two trends:
1) pruning-based methods with learned reduction patterns are consistently among the top-3 methods across all datasets and 2) soft-clustering methods are consistently among the bottom three methods across all datasets.

We also find that with the DeiT-T and DeiT-S backbones the hard-merging methods ToMe and DPC-KNN regularly outperform all other methods, especially on the ImageNet dataset and when only 25-50% of the tokens are kept.

However, with the DeiT-B backbone, we observe that the pruning-based methods with learned reduction patterns outperform even the hard-merging methods. Looking closer into the pruning-based methods we observe that the fixedpattern ℓp methods are competitive when 90% of tokens are kept, but at lower keep rates the performance drops significantly. For the learned approaches, we find that the DynamicViT method is the most unstable of the tested methods, often being in the bottom three methods when the keep rate is lower than 90%. Similarly, we find that the performance of the ATS method is very dataset dependent, with great performance on the challenging NUS-WIDE dataset, but average performance on all other datasets. It should be noted that the ATS method manages to do so while on average using 50-90 and 10-30 fewer tokens than the other methods when the keep rate is set to 90% and 70%, respectively. This is discussed in the supp. mat.

Comparatively, with a keep rate of 50-90% the Top-K
method is the best performing method 36% of the time and in the top-3 methods 83% of the time. This contradicts pre-
Table 1: **Performance of Token Reduction methods with DeiT-S backbone.** Model performance is measured across varying keep rates, r, denoted in percentage of tokens kept at each reduction stage. Scores exceeding the DeiT baseline are noted in **bold**, measured in Top-1 accuracy for ImageNet & NABirds and mean Average Precision for COCO & NUSWIDE. The three best performing methods per keep rate are denoted in descending order with red , orange , and yellow ,
respectively. Similarly, the three worst performing methods are denoted in descending order with light blue , blue , and dark blue . Results with the DeiT-B and DeiT-T backbones are available in the supp. mat.

| ImageNet    | NABirds   | COCO   | NUS-WIDE   |       |       |       |       |       |       |       |       |       |       |       |       |       |
|-------------|-----------|--------|------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| DeiT-S      | 79.85     | 80.57  | 78.11      | 63.23 |       |       |       |       |       |       |       |       |       |       |       |       |
| r (%)       | 25        | 50     | 70         | 90    | 25    | 50    | 70    | 90    | 25    | 50    | 70    | 90    | 25    | 50    | 70    | 90    |
| ℓ1          | 70.05     | 74.47  | 77.25      | 79.17 | 62.90 | 70.52 | 77.10 | 80.08 | 61.28 | 69.49 | 74.31 | 77.09 | 54.44 | 59.60 | 61.98 | 62.91 |
| ℓ2          | 70.54     | 74.86  | 77.41      | 79.27 | 64.28 | 72.09 | 77.53 | 80.11 | 62.23 | 70.30 | 74.66 | 77.19 | 55.31 | 60.22 | 62.07 | 62.71 |
| ℓ∞          | 70.58     | 74.03  | 77.48      | 79.23 | 63.36 | 70.19 | 77.23 | 79.96 | 61.50 | 69.11 | 74.73 | 77.27 | 55.10 | 59.34 | 62.11 | 62.77 |
| Top-K       | 72.91     | 77.82  | 79.22      | 79.87 | 76.28 | 80.38 | 80.70 | 80.60 | 70.14 | 75.84 | 77.50 | 78.09 | 59.32 | 61.98 | 62.69 | 63.26 |
| EViT        | 74.17     | 78.08  | 79.30      | 79.87 | 76.74 | 80.28 | 80.73 | 80.64 | 71.28 | 75.78 | 77.50 | 78.07 | 59.69 | 61.89 | 62.67 | 63.25 |
| DynamicViT  | 60.32     | 77.84  | 79.17      | 79.79 | 70.60 | 80.62 | 80.77 | 80.84 | 39.18 | 69.02 | 75.43 | 77.69 | 39.20 | 57.83 | 61.96 | 63.16 |
| ATS         | 72.95     | 77.86  | 79.09      | 79.63 | 73.46 | 78.89 | 80.36 | 80.55 | 70.13 | 75.66 | 77.23 | 77.83 | 60.20 | 62.35 | 62.93 | 63.18 |
| ToMe        | -         | 78.29  | 79.63      | 79.92 | -     | 74.99 | 80.05 | 80.68 | -     | 74.99 | 77.36 | 77.88 | -     | 61.51 | 62.50 | 62.89 |
| K-Medoids   | 68.94     | 76.44  | 78.74      | 79.73 | 65.28 | 76.95 | 79.75 | 80.46 | 66.26 | 74.15 | 76.76 | 77.94 | 57.78 | 61.48 | 62.47 | 63.12 |
| DPC-KNN     | 75.01     | 77.95  | 78.85      | 79.54 | 68.77 | 74.14 | 76.70 | 78.88 | 72.15 | 75.70 | 77.06 | 77.74 | 60.78 | 62.11 | 62.67 | 62.93 |
| SiT         | 74.65     | 77.16  | 77.52      | 77.71 | 62.82 | 62.02 | 60.72 | 58.50 | 57.65 | 57.33 | 57.11 | 57.13 | 57.95 | 58.84 | 59.29 | 59.59 |
| PatchMerger | 69.44     | 74.17  | 75.80      | 76.75 | 47.26 | 61.34 | 65.45 | 68.24 | 62.24 | 68.09 | 70.75 | 72.12 | 55.82 | 59.27 | 60.46 | 61.20 |
| Sinkhorn    | 64.26     | 64.07  | 64.02      | 64.09 | 48.89 | 50.19 | 51.46 | 51.22 | 56.93 | 56.68 | 56.85 | 56.65 | 50.59 | 50.67 | 50.63 | 50.21 |

vious results [18], and indicates that the Top-K method is a very strong baseline. However, at a keep rate of 25% we find that the fused tokens in the EViT method can lead to an improvement of up to 2 percentage points over the Top-K
method. This is also evident in Figure 1, where on average the EViT and Top-K methods are the two best ranked methods. Lastly, we note that when 90% of tokens are kept, the Top-K, EViT, DynamicViT, ATS, and ToMe methods outperform the DeiT baselines by up to 0.5 percentage points.

## 5. In-Depth Analysis Of Reduction Patterns

In order to gain deeper insights into the token reduction process, we pose a set of research questions dedicated to uncovering the underlying core mechanisms of the investigated methods. We calculate the defined metrics per dataset and aggregate across all datasets, unless otherwise noted.

Per-dataset results and examples of token reduction patterns can be found in the supp. mat.

## 5.1. Are Reduction Patterns Consistent When Varying The Keep Rate R?

A common assumption is that the token reduction methods will select tokens from the most representative regions of the image [50]. Assuming this to be true, one would expect that the reduction patterns are consistent (i.e. the same set of tokens are merged or pruned) when: 1) reducing the keep rate r and 2) when varying the backbone capacity (see Section 5.2).

In order to evaluate whether the reduction patterns are consistent under varying keep rates, we consider reduction patterns M1 and M2 from models trained with keep rates r1 and r2, respectively, where r1 ̸= r2. We only compare within the same reduction method and backbone capacity.

For pruning-based methods we evaluate using the Intersection over Area (IoA) between the reduction patterns, i.e.

how large a ratio of the tokens in M2 are present in M1, assuming r1 > r2. For merging-based methods we evaluate using the Homogeneity of the constructed clusters [53].

Homogeneity is a measure of how consistent the class assignment is within each cluster, i.e. whether the elements of each cluster in M1 are assigned to the same clusters in M2, assuming r1 > r2. Further details on IoA and Homogeneity can be found in the supp. mat.

For the hard-clustering methods DPC-KNN and KMedoids we evaluate using IoA in addition to Homogeneity, by treating the cluster centers as kept tokens. For evaluation of soft-merging methods, we define M by assigning each token to the cluster with the highest assignment score.

We plot our findings in Figure 3. First we find that when lowering the keep rates, the IoA of Top-K, EViT, and DynamicViT (i.e. the fixed keep rate pruning methods) are consistently high. However, for the hard-clustering methods and the dynamic keep rate ATS we observe that the IoA
quickly drops across all reduction stages, towards the lower bound IoA values, indicating the extracted reduction patterns differ significantly. Secondly, we find that the Homogeneity of the hard-merging methods is consistently high,

![5_image_0.png](5_image_0.png)

IoA
Homogeneity
while it is significantly lower for the soft-merging methods.

From this we can conclude that pruning-based methods, with the exception of ATS, produce consistent reduction patterns when varying r. Similarly, we find that the hardmerging methods select consistent clusters, but with inconsistent cluster centers, while soft-merging methods produce inconsistent clusters when varying r.

## 5.2. Are Reduction Patterns Consistent When Varying Model Capacity?

In order to evaluate whether the reduction patterns are affected by the backbone model capacity, we consider reduction patterns M1 and M2 from the same token reduction method trained with r1 = r2 but varying backbone capacity.

For pruning-based methods we evaluate using Intersection over Union (IoU) to gauge the similarity of M1 and M2.

For merging-based methods we evaluate using the Normalized Mutual Information (NMI) [57]. Further details on IoU
and NMI can be found in the supp. mat.

As seen in Figure 4, we find that for pruning-based methods the similarity of the reduction patterns is very low for all keep rates. Similar to the observations made in Section 5.1 we observe that the IoU of the DPC-KNN, K-Medoids, and ATS methods is especially low. This indicates that the reduction patterns for pruning-based methods are inconsistent as the backbone model capacity is varied. However, for the hard-merging methods we observed that the clusters are consistent across backbone capacities at all reduction stages when r > 25%. The same can be observed for the soft-merging based PatchMerger method when r > 50%.

From this we can conclude that the reduction patterns of pruning-based methods are inconsistent when varying the backbone capacity. For merging-based methods we find that the ToMe, DPC-KNN, and K-Medoids methods are consistent as long as r > 25%, while PatchMerger is consistent for r > 50%. We can again conclude that the hard-merging methods select consistent clusters, but with varying cluster centers, while soft-merging methods produce inconsistent clusters, as was observed in Section 5.1.

## 5.3. Do Reduction Patterns Differ Across Datasets?

Little is known about the behaviour of the reduction patterns across different image datasets. One open question is whether there are strong commonalities in the reduction patterns from different datasets, or whether the reduction patterns differ across datasets. In order to do such a comparison, we have to consider the global trends, as per-example comparisons cannot be made. We denote the dataset averaged reduction pattern as M¯ , which is obtained by computing how many ViT stages each token is passed through, averaged over the validation data splits.

We evaluate the similarity of the averaged reduction patterns across datasets, M¯1 and M¯2, by considering reduction patterns from the same token reduction method trained with keep rate r and backbone capacity, but obtained from different datasets. In order to quantify the similarity we draw inspiration from the saliency detection field, specifically the analysis of different metrics by Bylinskii *et al*. [6]. We use the common Pearson's correlation coefficient, and report results with other common saliency metrics in the supp. mat.

As seen in Figure 5 and following the rule of thumb guidelines by Hinkle *et al*. [24], we find a moderate-to-high correlation of the averaged reduction patterns for nearly all methods across all datasets and keep rates. The exceptions are the DPC-KNN, K-Medoids, and DynamicViT methods, which are found to have spurious lower (but still positive)

![6_image_0.png](6_image_0.png)

![6_image_1.png](6_image_1.png)

Pearson'sCorrelation

![6_image_3.png](6_image_3.png)

![6_image_2.png](6_image_2.png)

correlation scores for several dataset pairs, indicating the averaged reduction patterns are less consistent. The lowest correlation scores are obtained by the DPC-KNN method, though this may be attributed to the inconsistent cluster centers as described in Section 5.1-5.2. However, it is not intuitive that the reduction patterns are highly correlated across datasets, as one would expect that due to the significant differences across the investigated datasets imposed by the difference in type of classification task and its granularity.

Nonetheless, the results indicate that on average the different token positions are used equally often across datasets.

This might be due to biases in the image capturing process e.g. the sky is always in the top half of the image and the object of interest is in the lower half and center of the image (as seen in Figure 2). We conclude that in general the reduction patterns do not differ significantly across datasets.

## 5.4. Do Pruning-Based Reduction Patterns Differ From Fixed Patterns?

As discovered in prior work [50, 70], when averaging reduction patterns across a dataset the tokens near the image center are kept for longer, resembling a radial selection function. Therefore, it is reasonable to question how similar the per-example reduction patterns are to the fixed patterns from the ℓp methods. If they are similar, one could in principle do away with learning the adaptive reduction patterns.

We find that all learned pruning-based reduction patterns have a very low IoU with the fixed ℓp patterns at all reduction stages, shown in detail in the supp. mat. As the ℓp patterns gradually focus on the tokens close to the image center, this indicates that the learned reduction patterns are not focused on the center. Instead the learned reduction patterns use information from across the entire image at all stages. We can therefore conclude that the learned pruningbased reduction patterns differ from fixed radial patterns.

![7_image_0.png](7_image_0.png)

## 5.5. Are Reduction Patterns Good Proxies For Model Performance?

A key practical question is whether a pair of reduction patterns can be used to predict the difference in model performance. This is investigated by determining the correlation between f(A) − f(B) and d(MA, MB), where f is a performance measure (i.e. accuracy or mAP), d is a similarity measure, A and B are two models, and MA and MB
are their reduction patterns. We use the Spearman's ranked correlation coefficient to measure the correlation. This approach was originally proposed by Ding *et al*. [14]. We set d to be IoU and NMI for pruning- and merging-based methods, respectively, and constrain A to be the Top-K and K-Medoids models as they are both strong baselines. Additionally, we measure the feature alignment between A and B by defining d to be the orthogonal Procrustes distance, which Ding *et al*. [14] found to be a better metric of feature alignment than the typically used Centered Kernel Alignment [31] and Projection-Corrected Canonical Correlation Analysis [47]. For the feature alignment test we allow A
to be Top-K, K-Medoids, and DeiT, and calculate the alignment using the CLS token after the three reduction stages and the final ViT stage.

We find that for all model capacities the orthogonal Procrustes distance and NMI are highly correlated with the difference in model performance, while the IoU metric is moderately correlated, see Figure 6. The fact that NMI is a better proxy than IoU indicates that the merging-based methods are more sensitive to the construction of the clusters, whereas pruning-based methods are less sensitive to the selection of specific tokens.

Lastly, the reason the Procrustes distance is a good proxy may be grounded in the fact all tested methods use a pretrained DeiT backbone. Therefore, as long as the CLS token does not change during training of the token reduction method, feature alignment should be a good proxy. This has previously been the motivation for distillation losses
[50, 78]. However, all tested methods were trained without such losses, indicating that the well-performing models have inherently retained high CLS token alignment.

## 6. Limitations

We deliberately set certain limitations in order to keep the experiment complexity manageable. First of all, we only consider the image classification task where we extend the analysis from just ImageNet to three additional datasets.

Therefore, extending our analysis to additional tasks such as video classification and action recognition is seen as out of scope and we leave this for the future. Secondly, we restricted our training scheme to only consider an ImageNet pre-trained backbone. This is common practice in the literature. Training from scratch on datasets such as ImageNet, OpenImages [33], or Visual Genome [32] would have been prohibitive, and we consider the fine-tuning setting to be more realistic when generalizing to datasets other than ImageNet. Thirdly, we do not investigate how interpretable or robust the different token reduction methods are, though this would be of great interest in the future. Lastly, while the main motivation for the token reduction methods has been to make ViTs more efficient, this work does not evaluate the efficiency of the tested methods. This is a deliberate choice as this work focuses on establishing a systematic comparison of methods with regards to classification performance, as well as gaining deeper insights into the core mechanisms of the tested methods. Furthermore, efficiency is not a simple thing to measure due to confounding factors such as hardware, implementation, and infrastructure as discussed by Dehghani *et al*. [12].

## 7. Conclusion

In this work we presented the first comprehensive and systematic analysis of 10 state-of-the-art token reduction methods. We find that the simple Top-K pruning approach is a very strong baseline across all tested image classification datasets, only outperformed by the EViT method, a straight-forward extension of Top-K. We conducted the first analysis of how the reduction patterns are affected by choice of dataset, number of tokens to be kept, and model capacity.

We observe that varying the backbone has a large effect on the reduction patterns, whereas when the keep rate is varied the reduction patterns are very consistent. We also found a moderate-to-strong correlation of the average reduction patterns across datasets, and that the similarity of reduction patterns between methods is a moderate-to-strong proxy for model performance. We hope these findings will help inform future research in token reduction methods.

Acknowledgements: This work was supported by the Pioneer Centre for AI (DNRF grant number P1), partially supported by the Spanish project PID2022-136436NB-I00 and by ICREA under the ICREA Academia programme, and the Canada CIFAR AI Chairs.

## A. Overview Of Supplementary Materials

In these supplementary materials we describe in further detail aspects of the training process, performance of the tested models, per-dataset results of the in-depth analysis, and visual examples of the reduction patterns. We refer to both the relevant sections in the main manuscript as well as sections in the supplementary materials. Specifically the following will be described:

- Hyperparameters for the four classification datasets
(Section 3.3 / B).

- Performance of token reduction methods using the DeiT-T and DeiT-B backbones (Section 4 / C).

- Per-dataset analysis of the dynamic keep rate in the ATS method (Section 4 / D).

- Description of the pattern reduction similarity measures (Section 5.1–5.2 / E).

- Description of the lower bound IoA and IoU computation (Section 5.1–5.2 / F).

- Per-dataset results for varying the keep rate r and backbone capacity (Section 5.1–5.2 / G).

- Additional saliency metrics for the cross-dataset reduction pattern comparison (Section 5.3 / H).

- Results of the ℓp reduction pattern comparison (Section 5.4 / I).

- Results using CKA and PWCCA as proxies of model performance (Section 5.5 / J).

- Per-dataset scatter plots of the proxy measures and model performance (Section 5.5 / K).

- Per-dataset visualization of the averaged reduction patterns (Section 5.3 / L).
Table 2: **Hyperparameter grid search.** We conduct a gird search over a subset of the hyperparameters. For ImageNet the search is conducted over the token reduction methods (restricted to the warmup epochs, backbone scale, and how many the backbone weights are frozen), whereas for NABirds, COCO, and NUS-WIDE it is conducted for the DeiT-S baseline. We note that for the ImageNet dataset we restrict the backbone LR scale factor to only 1 or 0.01, following Rao *et al*. [50].

| Hyperparameter                    | Grid Values    |
|-----------------------------------|----------------|
| LR Normalization Factor (LR-Norm) | [512, 1024]    |
| Warmup Epochs (W-E)               | [5, 20]        |
| Backbone LR Scale (B-LR)          | [1, 0.1, 0.01] |
| Backbone Freeze Epochs (B-FE)     | [0, 5]         |

Learning Rate (LR) [0.01, 0.001, 0.0001]

LR Normalization Factor (LR-Norm) [512, 1024]

Warmup Epochs (W-E) [5, 20]

Backbone LR Scale (B-LR) [1, 0.1, 0.01] Backbone Freeze Epochs (B-FE) [0, 5]

- Per-dataset example visualization of the reduction patterns (Section 5 / M).

## B. Hyperparameters

In this section we further elaborate on the training details in Section 3.3 and describe the hyperparameters used during training in detail. The hyperparameters can be split into two groups: 1) the static hyperparameters per dataset and 2) the hyperparameters which we conducted a search on per method. The static hyperparameters were selected based on what have been used in prior methods applied on each dataset [22, 36, 50, 52], as well as training guidelines from the DeiT paper [59]. For all datasets we used the AdamW
optimizer [44] with a momentum of 0.9 weight decay of 0.05, a Cosine learning rate schedule [43] with a decay rate of 0.1, and stochastic depth of 0.1 [28]. We train all methods on 2 V100 GPUs with mixed precision, repeated augmentations (x3) [3, 25], and gradient accumulation if the batch cannot fit onto the GPUs. For the K-Medoids and Sinkhorn methods we perform three iterations for the clustering, set the entropy regularization ϵ in the Sinkhorn method to 1, and set the number of neighbours k = 5 for the DPC-KNN
method. The remaining hyperparameters are shown in Table 3.

For a subset of the hyperparameters we perform a grid search per token reduction method with the DeiT-S backbone on the ImageNet dataset, and for the DeiT-S baseline on the NABirds, COCO, and NUS-WIDE datasets. The grid searched hyperparameters are: the learning rate, the number of warmup epochs in the cosine scheduler, the number of epochs where the backbone weights should be fixed, the backbone weights learning rate scaling factor, and a normalization factor of the learning rate [19]. The hyperparameter value ranges are shown in Table 2. On ImageNet we fix the learning rate to 0.001 and the normaliza-

| Dataset         | ImageNet                                                                                                                    | NABirds                                                    | COCO                                                    | NUS-WIDE                                                |
|-----------------|-----------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------|
| Epochs          | 30                                                                                                                          | 50                                                         | 40                                                      | 40                                                      |
| Batch size      | 1024                                                                                                                        | 1024                                                       | 512                                                     | 512                                                     |
| Loss            | Cross-Entropy                                                                                                               | Cross-Entropy                                              | ASL [52]                                                | ASL [52]                                                |
| Label Smoothing | 0.1                                                                                                                         | 0                                                          | 0                                                       | 0                                                       |
| ASL γ−          | -                                                                                                                           | -                                                          | 4                                                       | 4                                                       |
| ASL Clip        | -                                                                                                                           | -                                                          | 0.05                                                    | 0.05                                                    |
| Model EMA       | 0.9999                                                                                                                      | 0.9999                                                     | 0.9997                                                  | 0.9997                                                  |
| Augmentations   | Random Resize and Crop Horizontal Flip (50%) RandAugment [11] Normalization Random Erasing (25%) [76] Mixup/CutMix [73, 75] | Random Resize and Crop Horizontal Flip (50%) Normalization | Resize Cutout (50%) [13] RandAugment [11] Normalization | Resize Cutout (50%) [13] RandAugment [11] Normalization |

Table 3: **Dataset-specific hyperparameters.** We fix a large set of the hyperparameters based on prior work. For ImageNet we are inspired by the DynamicViT and DeiT papers [50, 59], NABirds is based on the hyperparameters used in the TransFG
work [22], and COCO and NUS-WIDE are based on the hyperparameters from the ASL work [52]. Table 4: **Selected token reduction method hyperparameters - ImageNet.** We present the selected hyperparameters when searching on ImageNet for each token reduction method.

| r (%)       | 25   | 50   | 70   | 90   |      |     |      |      |     |      |      |    |
|-------------|------|------|------|------|------|-----|------|------|-----|------|------|----|
| W-E         | B-LR | B-FE | W-E  | B-LR | B-FE | W-E | B-LR | B-FE | W-E | B-LR | B-FE |    |
| ℓ1          | 5    | 1    | 0    | 5    | 0.01 | 5   | 5    | 0.01 | 5   | 5    | 0.01 | 5  |
| ℓ2          | 5    | 1    | 0    | 5    | 0.01 | 5   | 5    | 0.01 | 5   | 5    | 0.01 | 5  |
| ℓ∞          | 5    | 1    | 0    | 5    | 0.01 | 5   | 5    | 0.01 | 5   | 5    | 0.01 | 5  |
| Top-K       | 5    | 1    | 0    | 5    | 0.01 | 5   | 5    | 0.01 | 5   | 20   | 1    | 0  |
| EViT        | 5    | 1    | 0    | 5    | 0.01 | 5   | 5    | 0.01 | 5   | 20   | 1    | 0  |
| DynamicViT  | 20   | 0.01 | 5    | 5    | 0.01 | 5   | 20   | 0.01 | 5   | 20   | 0.01 | 5  |
| ATS         | 5    | 1    | 0    | 5    | 0.01 | 5   | 20   | 0.01 | 5   | 5    | 0.01 | 5  |
| ToMe        | -    | -    | -    | 5    | 0.01 | 5   | 5    | 0.01 | 5   | 5    | 0.01 | 5  |
| K-Medoids   | 5    | 1    | 0    | 5    | 0.01 | 5   | 20   | 0.01 | 5   | 20   | 1    | 0  |
| DPC-KNN     | 5    | 0.01 | 5    | 20   | 0.01 | 5   | 5    | 0.01 | 5   | 5    | 0.01 | 5  |
| SiT         | 5    | 0.01 | 5    | 5    | 0.01 | 5   | 5    | 0.01 | 5   | 20   | 0.01 | 5  |
| PatchMerger | 5    | 0.01 | 5    | 5    | 0.01 | 5   | 5    | 0.01 | 5   | 5    | 0.01 | 5  |
| Sinkhorn    | 5    | 1    | 0    | 5    | 1    | 0   | 5    | 1    | 0   | 5    | 1    | 0  |

Table 5: **Selected DeiT baseline hyperparameters.** We present the selected hyperparameters for the DeiT baselines on NABirds, COCO, and NUS-WIDE.

| Dataset   | LR     | LR-Norm   | W-E   | B-LR   | B-FE   |
|-----------|--------|-----------|-------|--------|--------|
| NABirds   | 0.001  | 1024      | 5     | 0.1    | 5      |
| COCO      | 0.0001 | 512       | 5     | 1      | 0      |
| NUS-WIDE  | 0.0001 | 512       | 5     | 1      | 0      |

tion factor to 1024 (i.e. the batch size). On the NABirds, COCO, and NUS-WIDE datasets we determine the final per-method hyperparameters by comparing models trained with the method-specific hyperparameters obtained on ImageNet and the dataset-specific DeiT-S baseline hyperparameters. The best hyperparameters per token reduction method and keep rate r on the ImageNet dataset is shown in Table 4.

For NABirds, COCO, and NUS-WIDE we show the best hyperparameters for the DeiT-S baseline in Table 5, and an indicator matrix in Table 6 indicating whether the dataset fine-tuned DeiT-S hyperparameters or the ImageNet hyper-
Table 6: **Hyperparameter indicator matrix.** We illustrate below for each method and keep rate r whether the hyperparameter settings from the ImageNet dataset (I) or the dataset specific DeiT-S baseline (D) are used.

| NABirds     | COCO   | NUS-WIDE   |    |    |    |    |    |    |    |    |    |    |
|-------------|--------|------------|----|----|----|----|----|----|----|----|----|----|
| r (%)       | 25     | 50         | 70 | 90 | 25 | 50 | 70 | 90 | 25 | 50 | 70 | 90 |
| ℓ1          | I      | D          | D  | D  | D  | D  | D  | D  | D  | D  | D  | D  |
| ℓ2          | I      | D          | D  | D  | D  | D  | D  | D  | D  | D  | D  | D  |
| ℓ∞          | I      | D          | D  | D  | D  | D  | D  | D  | D  | D  | D  | D  |
| Top-K       | D      | D          | D  | D  | D  | D  | D  | D  | D  | D  | D  | D  |
| EViT        | D      | D          | D  | D  | D  | D  | D  | D  | D  | D  | D  | D  |
| DynamicViT  | I      | D          | D  | D  | I  | I  | D  | D  | I  | D  | D  | D  |
| ATS         | I      | D          | D  | D  | D  | D  | D  | D  | D  | D  | D  | D  |
| ToMe        | -      | D          | D  | D  | -  | D  | D  | D  | -  | D  | D  | D  |
| K-Medoids   | D      | D          | D  | D  | D  | D  | D  | D  | D  | D  | D  | D  |
| DPC-KNN     | D      | D          | D  | D  | D  | D  | D  | D  | D  | D  | D  | D  |
| SiT         | D      | D          | D  | D  | D  | D  | D  | D  | I  | I  | I  | I  |
| PatchMerger | D      | D          | D  | D  | D  | D  | D  | D  | D  | D  | D  | D  |
| Sinkhorn    | I      | I          | I  | I  | I  | I  | I  | I  | I  | I  | I  | I  |

parameters are used per token reduction method and r.

can then be defined as in Equations 1-2.

## C. Token Reduction Performance Using Deit-T And Deit-B Backbones

In this section we present the results with the DeiT-T and DeiT-B baselines as mentioned in Section 4; see Table 7.

## D. Analysis Of Ats Keep Rates

As discussed in Section 3.1.3, the ATS [18] method is a dynamic keep rate pruning method, and therefore the meaning of the keep rate r makes a subtle but important change.

Instead of being the ratio of kept tokens, it instead represents the upper bound of the ratio of tokens to be kept, which the ATS method cannot exceed. In order to better understand the ATS method we plot the per-dataset average keep rate at each ViT stage for the different values of r; see Figure 7. We observe that when 90% and 70% of the tokens may be kept the actual keep rate is much lower, especially during the later stages of the ViT.

## E. Reduction Pattern Similarity Metrics

In this section we describe in more detail the metrics used to compare reduction patterns in Sections 5.1–5.2.

For the Intersection over Area (IoA) and Intersection over Union (IoU) metrics used to compare pruning-based methods, each method produces a reduction pattern M with keep rate r, where M consists of the kept tokens after applying the reduction method. Using set notation the IoA and IoU

$$\text{IoA}=\frac{M_{1}\cap M_{2}}{M_{2}}\text{s.t.}r_{1}\geq r_{2}\tag{1}$$  $$\text{IoU}=\frac{M_{1}\cap M_{s}}{M_{1}\cup M_{2}}\tag{2}$$  For clustering-based methods, we utilize two informa
(1)  $\frac{1}{2}$  (2)  ... 
(3)  $$\begin{array}{l}~~~~~~~~~~~~~~\end{array}$$ (4)  $$\begin{array}{l}~~~~~~~~~~~~~~\end{array}$$ (5)  . 
For clustering-based methods, we utilize two information theoretic metrics: Homogeneity [53] and Normalized Mutual Information (NMI) [57]. Homogeneity measures the class distribution within the constructed clusters, where the optimal value is obtained if all data points from the same class are assigned to the same cluster. This can be expressed using entropy as in Equations 3-5.

 $h=\left\{\begin{array}{cl}1&\mbox{if}H(C,K)=0\\ 1-\frac{H(C|K)}{H(C)}&\mbox{else}\end{array}\right.$  $H(C|K)=-\sum_{k=1}^{|K|}\sum_{c=1}^{|C|}\frac{n_{c,k}}{N}\log\frac{n_{c,k}}{n_k}$  $H(C)=-\sum_{c=1}^{|C|}\frac{n_c}{N}\log\frac{n_c}{N}$  $K$ is the set of generated clusters, $C$ is the set. 
where K is the set of generated clusters, C is the set of ground truth classes, nc,k is the number of data points from class c in cluster k, nc is the number of data points in class c, nk is the number of data points in cluster k, and N is the total number of data points.

In our analysis we define C to be the constructed clusters in M1 and K to be the clusters in M2, given r1 ≥ r2.

Table 7: **Performance of Token Reduction methods.** Measured across varying keep rates, r, and backbone capacities.

Scores exceeding the DeiT baseline are noted in **bold**, measured as Top-1 accuracy for ImageNet & NABirds and mean Average Precision for COCO and NUS-WIDE. The three best performing methods per keep rate are denoted in descending order with red , orange , and yellow , respectively. Similarly, the three worst performing methods are denoted in descending order with light blue , blue , and dark blue

| (a) Performance comparison of token reduction methods trained with a DeiT-Base backbone. ImageNet NABirds COCO   | NUS-WIDE   |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |
|------------------------------------------------------------------------------------------------------------------|------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| DeiT-B                                                                                                           | 81.85      | 83.32 | 80.93 | 64.37 |       |       |       |       |       |       |       |       |       |       |       |       |
| r (%)                                                                                                            | 25         | 50    | 70    | 90    | 25    | 50    | 70    | 90    | 25    | 50    | 70    | 90    | 25    | 50    | 70    | 90    |
| ℓ1                                                                                                               | 71.23      | 74.96 | 78.94 | 81.04 | 59.79 | 71.57 | 78.92 | 82.42 | 58.28 | 69.27 | 76.23 | 79.65 | 53.01 | 60.10 | 63.25 | 64.14 |
| ℓ2                                                                                                               | 71.41      | 75.40 | 79.07 | 81.18 | 61.55 | 73.24 | 79.52 | 82.55 | 59.69 | 70.33 | 76.56 | 79.75 | 54.00 | 60.37 | 63.29 | 64.28 |
| ℓ∞                                                                                                               | 71.67      | 74.40 | 78.95 | 81.20 | 59.96 | 70.51 | 79.73 | 82.59 | 58.48 | 68.50 | 76.54 | 79.89 | 53.00 | 59.59 | 63.12 | 64.25 |
| Top-K                                                                                                            | 73.63      | 78.97 | 80.91 | 82.03 | 74.71 | 82.22 | 83.20 | 83.40 | 67.63 | 76.91 | 79.95 | 80.97 | 58.51 | 62.78 | 63.92 | 64.40 |
| EViT                                                                                                             | 75.26      | 79.22 | 80.99 | 82.00 | 74.73 | 82.00 | 83.19 | 83.33 | 68.93 | 76.92 | 79.87 | 80.92 | 59.00 | 62.88 | 63.90 | 64.43 |
| DynamicViT                                                                                                       | 27.94      | 74.58 | 80.68 | 81.76 | 49.23 | 82.30 | 83.16 | 83.23 | 24.88 | 62.79 | 76.54 | 80.64 | 28.56 | 55.51 | 60.73 | 63.83 |
| ATS                                                                                                              | 73.89      | 78.94 | 80.78 | 81.57 | 71.00 | 80.10 | 82.58 | 83.26 | 68.17 | 76.38 | 79.35 | 80.50 | 59.49 | 63.17 | 64.21 | 64.48 |
| ToMe                                                                                                             | -          | 78.89 | 81.05 | 82.00 | -     | 73.67 | 81.59 | 82.98 | -     | 74.11 | 78.82 | 80.48 | -     | 62.38 | 64.06 | 64.35 |
| K-Medoids                                                                                                        | 69.12      | 76.86 | 79.98 | 81.76 | 57.54 | 75.29 | 80.62 | 82.57 | 61.79 | 73.60 | 77.58 | 80.32 | 56.67 | 62.18 | 63.53 | 64.35 |
| DPC-KNN                                                                                                          | 69.40      | 75.87 | 79.06 | 81.05 | 58.16 | 67.36 | 72.83 | 78.29 | 65.99 | 73.32 | 77.03 | 79.76 | 58.58 | 61.39 | 62.96 | 63.87 |
| SiT                                                                                                              | 68.39      | 75.53 | 76.63 | 77.26 | 65.09 | 70.75 | 70.36 | 68.96 | 54.86 | 53.27 | 53.16 | 52.73 | 56.12 | 59.76 | 60.64 | 61.08 |
| PatchMerger                                                                                                      | 58.78      | 70.63 | 74.52 | 76.76 | 40.38 | 57.21 | 62.20 | 67.06 | 54.25 | 66.22 | 70.97 | 73.72 | 51.80 | 58.83 | 60.79 | 62.09 |
| Sinkhorn                                                                                                         | 63.37      | 63.33 | 63.36 | 63.50 | 42.89 | 42.33 | 41.72 | 42.86 | 52.57 | 52.33 | 52.21 | 52.12 | 47.55 | 47.41 | 47.26 | 47.48 |

(b) Performance comparison of token reduction methods trained with a DeiT-Tiny backbone.

ImageNet NABirds COCO NUS-WIDE

DeiT-T 72.20 74.16 71.09 59.27

r (%) 25 50 70 90 25 50 70 90 25 50 70 90 25 50 70 90

ℓ1 58.58 62.27 67.91 71.06 51.82 59.25 68.36 73.47 49.09 58.27 67.03 70.24 44.81 52.64 57.30 58.73 ℓ2 58.85 62.91 67.91 71.13 53.10 60.87 69.20 73.42 50.46 60.00 67.33 69.98 45.73 53.45 57.34 58.54 ℓ∞ 59.08 61.92 67.79 71.38 52.60 57.70 69.25 73.34 50.08 57.30 67.22 69.89 45.32 51.91 57.41 58.30

Top-K 62.19 68.55 70.96 71.85 62.14 73.19 74.57 **74.64** 60.31 67.47 70.20 **71.65** 52.20 57.02 58.60 **59.50**

EViT 64.11 68.69 71.06 71.83 64.13 73.24 74.49 **74.53** 61.44 67.62 70.25 **71.63** 53.09 57.26 58.64 **59.49** DynamicViT 36.93 67.40 70.94 72.14 57.38 72.54 73.97 **74.30** 24.67 61.70 68.83 **71.30** 28.09 49.36 56.79 58.95 ATS 62.63 68.61 70.77 71.71 64.53 71.07 73.71 **74.43** 60.97 67.37 69.88 **71.10** 52.85 57.30 58.55 59.20

ToMe - 69.72 71.74 72.16 - 66.61 73.65 **74.50** - 65.66 69.70 **71.16** - 55.32 57.78 58.98

K-Medoids 57.50 65.82 69.90 71.50 44.62 66.52 72.09 74.04 54.08 64.83 69.09 71.05 49.13 55.92 58.38 59.07 DPC-KNN 64.56 69.68 71.10 71.88 64.23 71.05 73.02 74.05 63.32 68.03 69.55 70.84 55.37 57.33 58.08 58.88

SiT 63.43 67.98 68.99 68.90 36.35 36.65 34.00 35.07 48.01 47.50 46.98 46.48 36.67 38.15 36.98 37.70

PatchMerger 60.38 64.80 66.81 68.09 38.83 54.20 59.94 62.60 52.49 59.69 62.63 64.30 47.56 52.69 54.33 55.37

Sinkhorn 53.61 53.49 53.19 53.51 36.94 35.98 37.29 36.19 50.47 49.52 49.12 49.01 45.77 44.81 44.52 44.20

Thereby |K*| ≤ |*C|, and the Homogeneity measures how well each cluster in M1 maps to the reduced amount of clusters in M2.

Similarly the NMI can be expressed using the Mutual Information (MI) between the constructed clusters in M1 and M2 (denoted as C and K to keep consistency with the Homogeneity notation), normalized by the averaged entropy of C and K, see Equations 6-7.

$$\operatorname{NMI}(C,K)={\frac{I(C,K)}{(H(C)+H(K))/2}}$$
$$I(C,K)={\frac{n_{c,k}}{N}}\log{\frac{N n_{c,k}}{n_{c}n_{k}}}$$
$\downarrow$ . 

 \label {eq:MI} I(C,K) = \frac {n_{c,k}}{N}\log \frac {Nn_{c,k}}{n_c n_k} (7)
$$(6)$$

Furthermore, using the Homogeneity h, and the symmetric metric "Completeness", c, a combined metric called the
"V-Measure" can be defined as the harmonic mean of h and c [53]. It has been shown by Becker [2] that the V-Measure is equivalent to the Normalized Mutual Information, when the arithmetic mean is used for normalizing the MI.

![12_image_0.png](12_image_0.png)

## F. Lower Bound Of Ioa And Iou

When comparing pruning-based token reduction methods in Section 5.1–5.2 the keep rates, r1 and r2, may be selected such that a subset of tokens will be selected by both models. This can skew the interpretation of the IoA and IoU metrics, as the metrics may have high values but in fact only due to the inherently overlapping subset. In order to account for this we determine the minimum IoA and IoU for the reduction stage given r1, r2, and number of spatial tokens in the input image, P, using the Algorithms 1-2.

These lower bounds are only true for pruning-based methods with a static keep rate and may therefore be broken by the ATS method.

It is not necessary to derive similar lower bounds for the clustering-based Homogeneity and NMI metrics, as both metrics can reach a value of 0. Homogeneity reaches 0 when the clustering provides no new information, i.e. when the class distribution in each cluster is equal to the overall class distribution [53]. Similarly, it can be inferred the same is true for the NMI, since NMI can be expressed in terms of Homogeneity and Completeness.

## G. Per-Dataset Results When Varying R And Backbone Capacity

In this section, we extend the analysis conducted in Sections 5.1–5.2 by presenting per-dataset results when testing the consistency of reduction patterns under varying keep rate r and backbone capacity; see Figures 8-11. For all datasets we find that fixed rate pruning-based reduction patterns are consistent when varying r, but inconsistent when varying the backbone capacity. We also observe that the hard-merging methods have a high Homogeneity when varying r indicating the constructed clusters are very consistent, while DPC-KNN and K-Medoids have a low IoU in-

| Algorithm 1 Lower bound of IoA Input: P, r1, r2, s.t. r1 ≥ r2 Output: LB LB ← ∅ for s ∈ {1, 2, 3} do Ps,r1 ← ⌊P rs ⌋ 1 Ps,r2 ← ⌊P rs ⌋ 2 Ps,r1,r2 ← Ps,r1 + Ps,r2 if Ps,r1,r2 ≥ P then LBs ← Ps,r1,r2−P Ps,r2 else LBs ← 0 end if LB ← LB ∪ LBs end for Algorithm 2 Lower bound of IoU Input: P, r1, r2 Output: LB LB ← ∅ for s ∈ {1, 2, 3} do Ps,r1 ← ⌊P rs ⌋ 1 Ps,r2 ← ⌊P rs ⌋ 2 Ps,r1,r2 ← Ps,r1 + Ps,r2 if Ps,r1,r2 ≥ P then Ps,r1,r2−P LBs ← P else LBs ← 0 end if LB ← LB ∪ LBs end for   |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

dicating varying cluster centers, similar to the observations made in Section 5.1–5.2. We also observe the Homogeneity to be lower for soft-merging methods for all datasets.

Similar to the observations made in Section 5.2, we found that the hard-merging method have consistent reduction patterns when varying the backbone as long as r is above 25%
and 50% for PatchMerger, while the constructed clusters are inconsistent for the Sinkhorn and SiT methods. These findings match the findings made when analyzing the data aggregated across datasets in Sections 5.1–5.2.

## H. Expanded Cross-Dataset Reduction Pattern Metric Suite

We extend our analysis of the cross-dataset pruningbased reduction patterns in Section 5.3, by reporting results when using additional metrics from the saliency domain [6].

Specifically, we report results using the Spearman's ranked correlation coefficient, Jensen-Shannon Divergence, Earth Mover's Distance, and histogram similarity; see Figure 12.

We observe that for all metrics there is a high similarity between reduction patterns from different datasets. Specifically, we note that the results observed when using the Earth's Mover Distance are similar to results obtained with all other metrics. This is noteworthy, as the Earth Mover's Distance is the only metric which incorporates the spatial distance between the tokens, whereas the other metrics interpret the reduction patterns as 1D distributions.

## I. Comparison Of Pruning-Based And ℓp **Reduction Patterns**

In this section, we present more detailed results of the comparison of learned reduction patterns and the ℓp reduction patterns in Section 5.4. We report the IoU between the different ℓp fixed pattern reduction methods and the learned pruning-based reduction methods as well as the DPC-KNN
and K-Medoids cluster centers; see Figure 13. It is clear that all methods have a low IoU score across all reduction stages for all three ℓp methods, indicating that the learned reduction patterns are very different from the fixed imagecentered radial patterns applied by the ℓp.

## J. Extended Feature Alignment Metric Suite - Cka And Pwcca Analysis

We extend the analysis of whether feature alignment is a good proxy for model performance conducted in Section 5.5, by considering the commonly used metrics:
Centered Kernel Alignment (CKA) [31] and ProjectionCorrected Canonical Correlation Analysis (PWCCA) [47].

We follow the procedure laid out by Ding *et al*. [14] and make pairwise comparisons between all methods to the three anchor methods: Top-K, K-Medoids, and the baseline DeiT. The results are presented in Figure 14, and we observe that the CKA and PWCCA metrics are as good proxies for model performance as the orthogonal Procrustes Distance, with no noticeable differences in the results.

## K. Model Performance Proxies - Scatter Plots

As described in Section 5.5, we find that reduction pattern similarity and CLS token feature alignment are moderate-to-strong proxies of model performance. We present scatter plots comparing the metric difference between the anchor model and all other models against the orthogonal Procrustes distance, IoU, and NMI; see Figures 15-29. Note that for the sake of brevity the results from different keep rates are plotted in the same plot, but do report separate results per backbone model capacities.

![14_image_0.png](14_image_0.png) ![14_image_1.png](14_image_1.png)

![15_image_0.png](15_image_0.png)

![15_image_1.png](15_image_1.png)

![16_image_0.png](16_image_0.png)

## L. Visualization Of Dataset Averaged Reduction Patterns

In this section, we present visualization of the dataset averaged reduction patterns used in Section 5.3 from the learned pruning-based methods as well as DPC-KNN and K-Medoids; see Figures 30-35. We observe that the reduction patterns of the Top-K and EViT methods are visually very similar, which is intuitive given both methods use the same pruning technique. Comparatively, we observe that the DynamicViT tends to more often select tokens closer to the image center, whereas Top-K and EViT instead select tokens along the border. This behavior is also exhibited by the ATS method, where we can also observe that the tokens in general have a lower average depth (i.e. the tokens are on average processed by fewer ViT layers) due to the dynamic keep rate nature of the ATS method. For the K-Medoids and DPC-KNN methods we see that the tokens are less centered on the image center. Instead the average depth of the tokens is much more uniform, corresponding to what we can visually observe (see Section M) as well as determined when investigating the reduction pattern consistency.

## M. Per-Dataset Reduction Pattern Visualization Of Randomly Selected Samples

As mentioned in Section 5, we present the reduction patterns obtained from a random sample of each dataset; see Figures 36-43. For brevity, we only show the reduction patterns obtained using the DeiT-S backbone. For the mergingbased token reduction methods we observe that the hardmerging methods extract clusters which appear to be semantically coherent, whereas the soft-merging based methods more often extract clusters that are either incoherent or collapsed to a single cluster.

For the pruning-based methods we observe that the TopK, EViT, and DynamicViT methods manage to focus in on the distinguishing features in a similar manner. Comparatively, the reduction patterns of the ATS method are distinctively different as they maintain a more global diversity of tokens, instead of larger coherent regions. Lastly, we observe the K-Medoids and DPC-KNN methods tend to not select tokens with distinctive features as cluster centers.

This makes sense as these tokens may not be the best representative of a larger region.

![17_image_0.png](17_image_0.png)

![18_image_0.png](18_image_0.png)

![18_image_1.png](18_image_1.png)

![19_image_0.png](19_image_0.png)

![19_image_1.png](19_image_1.png) ![20_image_0.png](20_image_0.png)

![20_image_1.png](20_image_1.png)

![21_image_0.png](21_image_0.png)

![21_image_1.png](21_image_1.png)

![22_image_0.png](22_image_0.png)

![22_image_1.png](22_image_1.png)

![23_image_0.png](23_image_0.png) ![23_image_1.png](23_image_1.png)

![24_image_0.png](24_image_0.png)

![24_image_1.png](24_image_1.png)

![25_image_0.png](25_image_0.png)

![25_image_1.png](25_image_1.png)

![26_image_0.png](26_image_0.png)

![26_image_1.png](26_image_1.png) ![26_image_2.png](26_image_2.png)

![27_image_0.png](27_image_0.png)

![27_image_1.png](27_image_1.png) ![27_image_2.png](27_image_2.png)

![28_image_0.png](28_image_0.png)

![29_image_0.png](29_image_0.png)

![29_image_1.png](29_image_1.png)

![30_image_0.png](30_image_0.png)

![31_image_0.png](31_image_0.png)

![32_image_0.png](32_image_0.png)

![33_image_0.png](33_image_0.png)

![33_image_1.png](33_image_1.png)

![34_image_0.png](34_image_0.png)

![35_image_0.png](35_image_0.png)

## References

[1] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. VATT:
Transformers for multimodal self-supervised learning from raw video, audio and text. In A. Beygelzimer, Y. Dauphin, P.

Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. 2
[2] Hila Becker. Identification and characterization of events in social media. 2011. 12
[3] Maxim Berman, Herve J ´ egou, Andrea Vedaldi, Iasonas ´
Kokkinos, and Matthijs Douze. Multigrain: a unified image embedding for classes and instances. arXiv preprint arXiv:1902.05509, 2019. 9
[4] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. Flexivit: One model for all patch sizes. arXiv preprint arXiv:2212.08013, 2022. 2
[5] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. In International Conference on Learning Representations, 2023. 1, 2, 3
[6] Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba, and Fredo Durand. What do different evaluation metrics tell ´
us about saliency models? IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(3), 2019. 6, 14, 17
[7] Arnav Chavan, Zhiqiang Shen, Zhuang Liu, Zechun Liu, Kwang-Ting Cheng, and Eric Xing. Vision transformer slimming: Multi-dimension searching in continuous optimization space. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022. 2
[8] Mengzhao Chen, Mingbao Lin, Ke Li, Yunhang Shen, Yongjian Wu, Fei Chao, and Rongrong Ji Cf-vit. A general coarse-to-fine method for vision transformer. *arXiv preprint* arXiv:2203.03821, 2022. 2
[9] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang. Chasing sparsity in vision transformers: An end-to-end exploration. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, *Advances in Neural Information Processing Systems*,
volume 34, pages 19974–19988. Curran Associates, Inc.,
2021. 2
[10] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yan-Tao Zheng. Nus-wide: A realworld web image database from national university of singapore. In Proc. of ACM Conf. on Image and Video Retrieval
(CIVR'09), Santorini, Greece., July 8-10, 2009. 3
[11] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le.

Randaugment: Practical automated data augmentation with a reduced search space. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18613–
18624. Curran Associates, Inc., 2020. 10
[12] Mostafa Dehghani, Yi Tay, Anurag Arnab, Lucas Beyer, and Ashish Vaswani. The efficiency misnomer. In International Conference on Learning Representations, 2022. 8
[13] Terrance DeVries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 10
[14] Frances Ding, Jean-Stanislas Denain, and Jacob Steinhardt.

Grounding representation similarity through statistical testing. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, *Advances in Neural Information Processing Systems*, 2021. 8, 14
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In *International Conference on Learning Representations*, 2021. 1, 2
[16] Zhiyang Dou, Qingxuan Wu, Cheng Lin, Zeyu Cao, Qiangqiang Wu, Weilin Wan, Taku Komura, and Wenping Wang. Tore: Token reduction for efficient human mesh recovery with transformer. *arXiv preprint arXiv:2211.10705*,
2022. 2
[17] Mingjing Du, Shifei Ding, and Hongjie Jia. Study on density peaks clustering based on k-nearest neighbors and principal component analysis. *Knowledge-Based Systems*, 99:135–
145, 2016. 3
[18] Mohsen Fayyaz, Soroush Abbasi Koohpayegani, Farnoush Rezaei Jafari, Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash, and Juergen Gall. Adaptive token sampling for efficient vision transformers. In *Proceedings of the European Conference* on Computer Vision (ECCV), 2022. 1, 2, 3, 5, 11
[19] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noord- ´
huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. *arXiv preprint* arXiv:1706.02677, 2017. 9
[20] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herve Jegou, and Matthijs Douze. Levit: A vision transformer in convnet's clothing for faster inference. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, October 2021. 2
[21] Joakim Bruslund Haurum, Meysam Madadi, Sergio Escalera, and Thomas B. Moeslund. Multi-scale hybrid vision transformer and sinkhorn tokenizer for sewer defect classification. *Automation in Construction*, 144:104614, 2022. 2, 3
[22] Ju He, Jie-Neng Chen, Shuai Liu, Adam Kortylewski, Cheng Yang, Yutong Bai, and Changhu Wang. Transfg: A
transformer architecture for fine-grained recognition. *Proceedings of the AAAI Conference on Artificial Intelligence*,
36(1):852–860, Jun. 2022. 2, 4, 9, 10
[23] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial dimensions of vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021.

2
[24] Dennis E Hinkle, William Wiersma, and Stephen G Jurs.

Applied statistics for the behavioral sciences, volume 663.

Houghton Mifflin college division, 2003. 6
[25] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8126–8135, 2020. 9
[26] Chao Hu, Liqiang Zhu, Weibin Qiu, and Weijie Wu. Data augmentation vision transformer for fine-grained image classification. *arXiv preprint arXiv:2211.12879*, 2022. 2
[27] Yunqing Hu, Xuan Jin, Yin Zhang, Haiwen Hong, Jingfeng Zhang, Yuan He, and Hui Xue. Rams-trans: Recurrent attention multi-scale transformer forfine-grained image recognition. *arXiv preprint arXiv:2107.08192*, 2021. 2
[28] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Proceedings of the European Conference on Computer Vision (ECCV), pages 646–661, Cham, 2016. Springer International Publishing. 9
[29] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In *International Conference on Learning Representations*, 2017. 3
[30] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao Tang, Minghai Qin, and Yanzhi Wang. Spvit: Enabling faster vision transformers via latency-aware soft token pruning. In Proceedings of the European Conference on Computer Vision (ECCV), 2022. 2
[31] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, *Proceedings of the 36th International Conference on Machine Learning*, volume 97 of Proceedings of Machine Learning Research, pages 3519–3529. PMLR, 09–
15 Jun 2019. 8, 14
[32] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. *International* Journal of Computer Vision, 123(1):32–73, Feb. 2017. 8
[33] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4. *International Journal of Computer Vision*, 128(7):1956–1981, Mar.

2020. 8
[34] Ling Li, David Thorsley, and Joseph Hassoun. Sait: Sparse vision transformers through adaptive token pruning. *arXiv* preprint arXiv:2210.05832, 2022. 2
[35] Zhexin Li, Tong Yang, Peisong Wang, and Jian Cheng. Qvit: Fully differentiable quantization for vision transformer.

arXiv preprint arXiv:2201.07703, 2022. 2
[36] Youwei Liang, Chongjian GE, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. EVit: Expediting vision transformers via token reorganizations. In *International Conference* on Learning Representations, 2022. 2, 3, 9
[37] Mingbao Lin, Mengzhao Chen, Yuxin Zhang, Ke Li, Yunhang Shen, Chunhua Shen, and Rongrong Ji. Super vision transformer. *arXiv preprint arXiv:2205.11397*, 2022. 2
[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence ´
Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Proceedings of the European Conference on Computer Vision (ECCV), Cham, 2014. Springer International Publishing. 3
[39] Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng Li, and Shuchang Zhou. Fq-vit: Post-training quantization for fully quantized vision transformer. In *Proceedings of the ThirtyFirst International Joint Conference on Artificial Intelligence, IJCAI-22*, pages 1173–1179, 2022. 2
[40] Yue Liu, Christos Matsoukas, Fredrik Strand, Hossein Azizpour, and Kevin Smith. Patchdropout: Economizing vision transformers using patch dropout. In *Proceedings of the* IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 3953–3962, January 2023. 2
[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 2
[42] Sifan Long, Zhen Zhao, Jimin Pi, Shengsheng Wang, and Jingdong Wang. Beyond attentive tokens: Incorporating token importance and diversity for efficient vision transformers. *arXiv preprint arXiv:2211.11315*, 2022. 2
[43] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. 9
[44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In *International Conference on Learning* Representations, 2019. 9
[45] Dmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan, Anish Prabhu, Mohammad Rastegari, and Oncel Tuzel. Token pooling in vision transformers for image classification. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 12–21, January 2023. 2, 3
[46] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. Adavit:
Adaptive vision transformers for efficient image recognition. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 12309–
12318, June 2022. 2
[47] Ari Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural networks with canonical correlation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. 8, 14
[48] Bowen Pan, Rameswar Panda, Yifan Jiang, Zhangyang Wang, Rogerio Feris, and Aude Oliva. Ia-redˆ2:
Interpretability-aware redundancy reduction for vision transformers. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.

Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 24898–
24911. Curran Associates, Inc., 2021. 2
[49] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, *Advances in Neural Information Processing Systems*, 2021. 1
[50] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, *Advances in Neural Information Processing Systems*, 2021. 1, 2, 3, 4, 5, 7, 8, 9, 10
[51] Cedric Renggli, Andre Susano Pinto, Neil Houlsby, Basil ´
Mustafa, Joan Puigcerver, and Carlos Riquelme. Learning to merge tokens in vision transformers. *arXiv preprint* arXiv:2202.12015, 2022. 2, 3
[52] Tal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi Zelnik-Manor.

Asymmetric loss for multi-label classification. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, October 2021. 4, 9, 10
[53] Andrew Rosenberg and Julia Hirschberg. V-measure: A
conditional entropy-based external cluster evaluation measure. In *Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)*, pages 410–420, Prague, Czech Republic, June 2007. Association for Computational Linguistics. 5, 11, 12, 13
[54] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. *International Journal of Computer Vision*,
115(3):211–252, Apr. 2015. 3, 4
[55] Michael S Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner: Adaptive space-time tokenization for videos. In A. Beygelzimer, Y.

Dauphin, P. Liang, and J. Wortman Vaughan, editors, *Advances in Neural Information Processing Systems*, 2021. 2
[56] Zhuoran Song, Yihong Xu, Zhezhi He, Li Jiang, Naifeng Jing, and Xiaoyao Liang. Cp-vit: Cascade vision transformer pruning via progressive sparsity prediction. *arXiv preprint* arXiv:2203.04570, 2022. 2
[57] Alexander Strehl and Joydeep Ghosh. Cluster ensembles —
a knowledge reuse framework for combining multiple partitions. *J. Mach. Learn. Res.*, 3:583–617, mar 2003. 6, 11
[58] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu, and Dacheng Tao. Patch slimming for efficient vision transformers. In *Proceedings of the IEEE/CVF*
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 12155–12164, 2022. 2
[59] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 10347–10357. PMLR, 18–24 Jul 2021. 4, 9, 10
[60] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In *Proceedings of the IEEE Conference* on Computer Vision and Pattern Recognition (CVPR), June 2015. 3
[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 30. Curran Associates, Inc., 2017. 2
[62] Matthew Walmer, Saksham Suri, Kamal Gupta, and Abhinav Shrivastava. Teaching matters: Investigating the role of supervision in vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1
[63] Jun Wang, Xiaohan Yu, and Yongsheng Gao. Feature fusion vision transformer for fine-grained visual categorization. In British Machine Vision Conference, 2021. 2
[64] Yunke Wang, Bo Du, and Chang Xu. Multi-tailed vision transformer for efficient inference. *arXiv preprint* arXiv:2203.01587, 2022. 2
[65] Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang. Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.

2
[66] Yu Wang, Shuo Ye, Shujian Yu, and Xinge You. R2trans:fine-grained visual categorization with redundancy reduction. *arXiv preprint arXiv:2204.10095*, 2022. 2
[67] Lemeng Wu, Xingchao Liu, and Qiang Liu. Centroid transformers: Learning to abstract with attention, 2021. 2
[68] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18134–18144, June 2022. 2
[69] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng Xu, and Xing Sun. Evo-vit: Slow-fast token evolution for dynamic vision transformer. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36, pages 2964–2972, 2022. 2
[70] Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-ViT: Adaptive tokens for efficient vision transformer. In *Proceedings of the IEEE/CVF*
Conference on Computer Vision and Pattern Recognition, 2022. 2, 7
[71] Hao Yu and Jianxin Wu. A unified pruning framework for vision transformers. *arXiv preprint arXiv:2111.15127*, 2021.

2
[72] Xiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei, Philip Torr, Wayne Zhang, and Dahua Lin. Vision transformer with progressive sampling. Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV), 2021. 2
[73] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, October 2019. 10
[74] Wang Zeng, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo, Wanli Ouyang, and Xiaogang Wang. Not all tokens are equal: Human-centric visual analysis via token clustering transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11101–
11111, 2022. 2, 3
[75] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In *International Conference on Learning Representations*, 2018. 10
[76] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. *Proceedings of the AAAI Conference on Artificial Intelligence*,
34(07):13001–13008, Apr. 2020. 10
[77] Yichen Zhu, Yuqin Zhu, Jie Du, Yi Wang, Zhicai Ou, Feifei Feng, and Jian Tang. Make a long image short: Adaptive token length for vision transformers. *arXiv preprint* arXiv:2112.01686, 2021. 2
[78] Zhuofan Zong, Kunchang Li, Guanglu Song, Yali Wang, Yu Qiao, Biao Leng, and Yu Liu. Self-slimmed vision transformer. In *Proceedings of the European Conference on Computer Vision (ECCV)*, 2022. 2, 3, 8